---
layout: post
cover: 'assets/images/cover1.jpg'
title:  "[CS224n] Lecture3 : GloVe: Global Vectors for Word Representation"
date:   2017-04-06
tags: NLP cs224n
subclass: 'post tag-NLP tag-CS224n'
categories: 'joongkyunKim'
navigation: True
---

*이번 강의부터는 최근 2017년 CS224d(2016)와 동일한 강좌인 CS224n 강의 비디오가 youtube에 공개되면서 좀 더 따끈따끈한 최신 내용을 반영하고자 CS224n 강의를 정리하도록 하겠다. 앞서 진행된 두 강좌에 대해서는 내용이 거의 비슷하고 순서정도만 변경되어있기 때문에 이어서 진행해도 큰 어려움이 없을것으로 본다*

이번 3강에서는 앞에서 배운 Word2vec과 이어서 GloVe란 알고리즘에 대해 알아보며, evaluation에 대해서 알아보도록 한다.
새롭게 CS224n 강의에 대해서 진행하기 때문에 기존의 CS224d 강의 youtube 링크와 다르다는 것을 주의하기 바란다!

* Lecture slide : [LectureSlide3]({{ site.url }}/downloads/lec3/LectureSlide3.pdf)
* Lecture note  : [Lecturenote2]({{ site.url }}/downloads/lec3/LectureNote2.pdf)
* Video Clip    : [3강][lec-3]

###### Lecture 2 Review
3강에 대한 내용을 언급하기 이전에 2강에서 진행된 Word2vec 내용과 Gradient 에 대해서 다시한번 상기시키도록 하겠다. (*2강은 cs224d 강의내용이었기 때문에 cs224n으로 바뀌면서 언급하지 못한 내용에 대해서도 간략하게 언급하도록 하겠다*)

지난 강의에서 다뤘던 것중에 가장 중요한 내용은 Word2vec에서 어떻게 벡터를 이용해서 단어를 예측하는지, 확률의 정의는 어떻게 되는지, 그리고 목적함수가 어떠한 형태인지, 이를 최적화하기위해 gradient를 어떻게 구하는지에 대해서 쭉 배워보았다. ([지난 강의][Lecture2])
지난 강의에서 중심단어를 놓고, 좌우 window 크기만큼 예측을 하고, 한칸씩 이동을하면서 전체 문서의 단어들을 예측하는 그림을 기억할 것이다. 이는 `Skip-grams` 이라는 알고리즘이다. 이는 `Skip-grams`는 다음과 같이 설명된다.
> Predict context	words	given	target	(position	independent)

그리고 각 단어를 예측할 확률과 최종 목적함수는 다음과 같다.

\begin{equation}
p(o | c) = \frac{exp(u_o^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)}
\end{equation}

\begin{equation}
J(\theta) = \frac{1}{T} \sum_{t=1}^T\sum_{-m \le j \le m, j\neq 0} \log p(w_{t+j}|w_t)
\end{equation}

여기서 o는 outside(or output) 단어의 index(id) 이며, c는 중심단어의 index를 나타낸다. 또한 $u$와 $v$는 각각 outside 단어의 중심단어 벡터와 중심단어의 outside 벡터를 나타낸다. 즉, 각 단어는 두개의 벡터를 가지고 있음을 알 수 있다.

위의 목적함수를 최적화 하는 과정에서 update를 하는데 `gradient descent` 방식을 쓴다고 언급했었다. 하지만 이 방식은 document 내의 모든 단어에 대해 위의 확률을 다 계산한 다음 update하는 방식을 취한다. 하지만 이 경우에는 단어의 수가 매우 많을 경우 아주 비효율적인 update방식이므로 이에 대한 대책으로 `stochastic gradient descent` 방법이다. 이는 파라미터인 $\theta$ 를 각각의 window 를 진행한다음 지속적으로 upgrade 하겠다는 방식이다.

![stochasticgd](downloads/lec3/stochasticgd.png)


###### Skip-gram model & negative sampling
이제부터는 3강에서 추가된 내용에 대해 정리해볼 시간이다. (1)번 식에서 우리는 분모에 해당하는 부분을 계산하는데 상당히 많은 시간과 비용이 들것이라고 쉽게 예상할 수 있다.(*모든 단어에 대해서 두 벡터의 내적을 계산해야하므로*)
그래서 이 강의에서는 `negative sampling` 이라는 방식을 추천한다. 여기서 `negative sampling` 이라는 방식은 실제 center word와 context word가 아닌 random word를 일부 선택하여 binary 로지스틱 모델링을 하는 방식이다. 아래의 새롭게 제시된 목적함수를 살펴보자

(paper from "Distributed Representations of Words and Phrases and their Compositionality"(Mikolov et al.2013))

\begin{equation}
J_{t}(\theta) = \log \sigma(u_{o}^T v_{c}) + \sum_{i}^k E_{j \sim P(w)}[\log \sigma(-u_{o}^T v_{c})]
\end{equation}

\begin{equation}
J(\theta) = \frac{1}{T}\sum_{t=1}^T J_{t}(\theta)
\end{equation}

여기서 $\sigma$ 는 일반적으로 우리가아는 시그모이드 함수를 나타낸다($ \sigma(x) = \frac{1}{1+e^{-x}}$)
위의 식을 살펴보면 시그모이드 함수가 로지스틱 함수이므로 위와같이 로지스틱 모델링이란 말이 성립이된다. 또한 뒷부분은 $P(w)$ 의 확률분포를 따르는 j라는 샘플의 시그모이드 함수의 기대값을 나타낸다.
위의 (3)번 식은 아래와 같이 좀 더 명확하게 나타낼 수 있다.

\begin{equation}
J_{t}(\theta) = \log \sigma(u_{o}^T v_{c}) + \sum_{j \sim P(w)}[\log \sigma(-u_{o}^T v_{c})]
\end{equation}

이며, $P(w) = U(w)^{3/4} / Z$ 로 random negative sampling 을 하는 확률분포는 Uniform 분포를 변형하여 만들어진 분포를 따른다.
위의 식을 해석하면 실제 center word와 context word 가 나올 확률자체는 maximize 시키고, 뒤에 있는 random word 에 따른 확률은 minimize 할려는 목적함수를 의미한다. (*참고 : $\sigma(-x) = 1-\sigma(x)*)
우리는 위의 과정을 통해서, 실제 (2)번 목적함수 계산을 위해 계산비용이 매우 높은 (1)식의 확률을 계산해야했지만, 아래와 같이 `negative sampling` 방식을 이용함으로써 일정 k개의 단어들에 대해서만 계산을 해주면된다. 일반적으로 k는 강의에서는 10개를 사용한다고 한다.

추가로, 앞서 설명한 `Skip-gram` 모델 이외에 제시된 모델은 `Continuous bag of words(CBOW)` 라는 모델로, `Skip-gram` 은 center word로 부터 주변의 각각의 단어들을 예측하는 모델이라면, `CBOW` 모델은 주변 단어들의 vector를 이용하여 center word를 예측하는 모델이다.(*이에 대한 설명은 따로 진행되진 않고 Assignment 에서 다뤄지는것으로 보인다*)

이렇게 `Skip-gram`을 이용하여 단어를 벡터로 나타내는 word2vec 방식은 단어들의 동시발생 그자체에 관심이 있는데, 동시발생 `빈도`에 관심을 가지는 방식이 앞서 2강에서 설명한 `window based co-occurence matrix` 를 이용하는 방식이다(*CS224n 에서는 이 뒤에 2강에서 언급한 window based Co-Occurrence matrix를 설명한다. 이는 2강에서 자세히 다뤘기때문에 생략하기로 한다*)

###### GLoVe : Global Vectors for Word Representation
이번에는 GLoVe 라는 알고리즘에 대해 알아보자. GLoVe는 앞서 2강에서 언급한 `window based co-occurence matrix` 를 이용한다. 아래 GLoVe의 목적함수 식을 보자.

\begin{equation}
J(\theta) = \frac{1}{2} \sum_{i,j=1}^W f(P_{ij})(u_i^Tv_j - \log P_{ij})
\end{equation}

$$
f(X_{ij})=
\begin{cases}
(\frac{X_{ij}}{x_{max}})^{\alpha} & \text{if $X_{ij} \le x_{max}$} \\
1 &\text{otherwise}
\end{cases}
$$

여기에서 $P_{ij}$는 co-occurence matrix에서의 i,j번째 원소를 말하며, GLoVe 모형은 위의 목적함수를 최대화하는 u,v 벡터를 찾고자한다. 이 모형은 트레이닝이 빠르고, 작은 corpus에서도 좋은 성능을 나타내서 많이 이용하는 알고리즘 중 하나이다.
이 GLoVe 모형은 `비유(Word Vector Analogy)` 에서 좋은 성능을 나타낸다. 예를 들어 man:woman :: king: ? 라고 되어있다면, 물음표자리에는 대부분의 사람들이 queen을 유추해낼 수 있을 것이다. 이러한 비유의 관계를 GLoVe 모형이 잘 나타낸다. 그리고 이러한 비유하는 것을 알아내는 식은 아래와 같이 코사인 유사도를 이용하여 제일 높은 유사도를 갖는 d를 찾는 것이다. (a:b :: c: ?)

\begin{equation}
d = arg \max_i \frac{(x_b-x_a+x_c)^T x_i}{ \Vert x_b-x_a+x_c \Vert}
\end{equation}

이와 같은 방식으로 아래와 같이 대응되는 단어의 쌍, Company-CEO, 원형-비교급-최상급과 같은 연결을 찾을 수 있다.(그림상에서 두 점(단어)를 잇는 점선의 방향들이 유사함을 알 수 있다). 또한 이러한 비유, 유추는 문법적인(syntactic) 관계와 의미적인(Semantic) 관계 유추에도 활용될 수 있다.

![glovevis](downloads/lec3/glovevis.png)
![gloveceo](downloads/lec3/gloveceo.png)
![glovesuper](downloads/lec3/glovesuper.png)

그리고 GLoVe 모델은 다른 여러 모델과 비교하여 정확도 측면에서도 좋은 성능을 보여준다. 아래에서 보듯이 training 데이터 사이즈가 커질수록 당연하게 정확도가 높아짐을 알 수 있고, 맨아래의 모형 파라미터에서는 GLoVe가 300차원의 42billion 단어수를 트레이닝했을때 정확도가 상당히 높음을 알 수 있다. 또한 이전에 배운 Skip-gram 모형보다도 트레이닝 속도가 더 빠른 결과를 보여준다.

![gloveaccuracy](downloads/lec3/gloveaccuracy.png)
![glovetrainingtime](downloads/lec3/glovetrainingtime.png)

###### Summary
새롭게 시작하는 CS224n 3강에서는 앞서 2강에서 배운 Word2vec Skip-gram 모형과 Count base 인 Window based co-occurence matrix를 이용한 GLoVe 모형에 대해서 알아봤다. Skip-gram 모형에서 계산 비용을 줄이기 위한 negative sampling도 언급이 되었고, GLoVe모형이 어떤 목적함수를 갖고, 이것이 실제로 어떠한 장점을 갖는 모형인지, 그리고 다른 모델과 비교하는 알고리즘의 장점에 대해서도 알아봤다. 다음 강의에서는 Word Classification과 Neural Network에 대해서 좀 더 알아보기로 하자. 끝~

[lec-3]: https://www.youtube.com/watch?v=ASn7ExxLZws&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=3&t=672s
[Lecture2]: https://joongkyunkim.github.io/cs224d-lecture2
