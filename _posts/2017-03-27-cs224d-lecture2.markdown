---
layout: post
cover: 'assets/images/cover2.jpg'
title:  "[CS224d] Lecture2 : Word2vec"
date:   2017-03-27
tags: NLP cs224d
subclass: 'post tag-NLP tag-cs224'
categories: 'joongkyunKim'
navigation: True
---


이번 포스팅에서는 lecture1:introduction 에 이어서 lecture2: Word vectors에 관해서 내용을 정리해보도록 한다. 우선 지난 시간에 했던 내용을 잠시 상기시켜보면, Natural Language Processing(이하 NLP)와 Deep Learning에 대해서 개략적인 내용을 살펴봤으며, 딥러닝을 NLP에 적용했을 때 기존의 해결 방식과 차이점을 비교해 보았다. 여기서 결정적으로 중요했던 부분은 딥러닝에 NLP를 적용하기 위해서는 글자 하나하나를 벡터의 형태로 나타내야 한다는 공통점을 보였다. 이번 강의에서는 어떻게 글자를 벡터로 나타내는지에 대한 내용이다.

* Lecture slide : [LectureSlide2]({{ site.url }}/downloads/lec2/LectureSlide2.pdf)
* Lecture note(lecture1과 동일)  : [Lecturenote1]({{ site.url }}/downloads/lec1/LectureNote1.pdf)
* Video Clip    : [2강][lec-2]

### Lecture2 : Word vectors

 컴퓨터가 단어의 의미를 나타내기 위해 활용했던 방법으로 WordNet과 같이 상위-하위어의 관계를 보여주는 분류체계(taxonomy)로 나타내거나 동의어 집합을 보여주는 것이 종종 사용되었다.
![WordNet](downloads/lec2/wordnet.png)
하지만 위의 방식에는 몇몇 문제점이 존재한다. 먼저,'good' 의 동의어로 나온 adept, expert 등등은 동의어이지만 문맥에 따라 같이 쓰일 수 없다.(`nuance`의 차이) 또한 사람들이 하나하나 수작업으로 매칭작업을 진행해줘야하며 단어간의 유사성 계산에 있어서 힘들 수 밖에 없다.

또한 문장 또는 문서에서 해당 단어가 나오는 부분에 1, 아닌 부분에 0으로 만드는 벡터의 경우(아래그림)는 대부분 0으로 구성된 벡터이다. 그래서 아래의 예제와 같이 `motel`과 `hotel`은 유사한 단어지만 `AND` 연산을 시행했을 때 0으로 아무 연관이 없는것 처럼 표현되기도 한다.
![onehot](downloads/lec2/onehot.png)

###### Window based cooccurence matrix

이러한 문제점을 해결하기 위해 제시된 방식이 표현하고자 하는 단어의 이웃(neighbors)를 표현하는 방식이다. 이를 이용해서 표현한 단어들의 벡터의 집합을 `window based cooccurence matrix` 라 한다. 아래와 같이 3개의 문장이 있다고 가정하자. 해당 예제에서는 window의 길이를 1로 하고, symmetric한 구조를 가진다. 이렇게되면 focus 하고 있는 단어의 좌/우 1개씩을 window로 설정하는 방식이다. 예를들어 `like`라는 단어를 보면, `like` 좌우 이웃된 단어들은 첫문장에서 `I`,`deep` 그리고 두번째 문장에서 `I`,`NLP` 이다. 이 결과가 아래의 매트릭스에서 counting 된것을 확인할 수 있다. 이렇게 계산된 matrix가 아래와 같고 이는 당연스럽게 symmetric한 matrix 일 수 밖에 없다.

> I like deep learning.<br>
I like NLP.<br>
I enjoy flying.

![cooccurence](downloads/lec2/cooccurence.png)

하지만, 위의 방식에는 큰 문제점이 있다! 단어 수에 따라 matrix의 크기가 기하급수적으로 늘어날 뿐 아니라, matrix의 차원(dimension)이 커지면서 계산시에 매우 비효율적일 수 있다. 이러한 문제를 해결하기 위해 `차원축소` 기법이 필요하다

###### 차원축소기법 : Singular Value Decomposition(SVD)
Singular Value Decomposition(이하 SVD) 는 특잇값 분해라고 말하며, 이는 선형대수에서 고유값 분해(eigenvalue decomposition)과 함께 아주 기본적이고 매우 많이 쓰이는 행렬분해 기법이다. $U$는 $AA^T$를 고유값분해(eigendecomposition)해서 얻어진 직교행렬(orthogonal matrix)로 $U$의 열벡터들을 $A$의 left singular vector라 부른다.또한 $V$는 $A^TA$를 고유값분해해서 얻어진 직교행렬로서 $V$의 열벡터들을 $A$의 right singular vector라 부른다.마지막으로, $\Sigma$는 $AA^T$, $A^TA$를 고유값분해해서 나오는 고유값(eigenvalue)들의 square root를 대각원소로 하는 m x n 직사각 대각행렬로 그 대각원소들을 $A$의 특이값(singular value)이라 부른다. (*좀더 자세한 내용은 아래 출처인 '다크프로그래머' 블로그를 참조하기 바란다. SVD에 대해 아주 꼼꼼하고 다방면으로 잘 정리된 블로그이다.*)

출처: http://darkpgmr.tistory.com/106 (다크프로그래머)

이제 다시 강의내용으로 돌아와서 아래의 SVD 분해 형태를 살펴보자
![SVD](downloads/lec2/SVD.png)
위의 케이스는 full SVD형태이지만 잘 쓰지 않는다. 아래와 같이 reduced SVD를 일반적으로 쓰게 되는데, 특잇값중에서 0이 아닌 것들을 모아서 좀 더 압축된 형태로 분해하게 된다. (*불필요한(덜 중요한) 정보를 감안하지 않는다고 생각하면 될 것 같다.*) 이와 같이 축약된 형태의 SVD는 데이터 압축 등에 종종 사용되니 알아두도록 하자!
최종적으로, 우리도 `window based cooccurence matrix` 를 축약된 SVD를 이용해서 차원 축소를 시킬것이다.

이 차원 축소 과정을 아래의 파이썬 코드로 실행해 볼 수 있다.

{% highlight python %}
##python 3.0 이상 버전입니다.
import numpy as np
import matplotlib.pyplot as plt

la = np.linalg
words = ["I", "like","enjoy","deep","learning","NLP","flying","."]
X = np.array([[0,2,1,0,0,0,0,0],
              [2,0,0,1,0,1,0,0],
              [1,0,0,0,0,0,1,0],
              [0,1,0,0,1,0,0,0],
              [0,0,0,1,0,0,0,1],
              [0,1,0,0,0,0,0,1],
              [0,0,1,0,0,0,0,1],
              [0,0,0,0,1,1,1,0]])

U, s, Vh = la.svd(X, full_matrices=False)

for i in range(len(words)):
    plt.text(U[i,0], U[i,1], words[i])

plt.axis([-0.8,0.2,-0.8,0.8])

plt.show()
print(U[0,])
{% endhighlight %}
이 코드의 결과는 아래와 같이 나온다.
![SVDresult](downloads/lec2/SVDresult.png)

$$ U[0,] =\begin{pmatrix} -0.52412493 \\\ -0.57285914 \\\ 0.0954463 \\\  0.38322849 \\\ -0.17696338 \\\ -0.17609218 \\\ -0.4191856 \\\ -0.05577027\end{pmatrix}
$$

위의 그림을 보면, U행렬을 첫열과 둘째열을 plot으로 나타낸것이다. 기하학적 의미를 간략하게 살펴보면 위에서 구한 `window based cooccurence matrix` 에서 단어간의 유사도를 살펴볼 수 있다. 이는 아래 그림의 예제에서 살펴볼 수 있는데, 동일하게 SVD를 통해서 plot한 결과가 같은 단어의 시제끼리 비슷한 위치에 존재하는 것을 알 수 있다.
![SVDexp](downloads/lec2/SVDexp.png)
출처 : An Improved Model of Seman4c Similarity Based on Lexical Co-Occurrence.Rohde et al. 2005

그리고 이렇게 SVD 과정을 거치게 되면 아래의 벡터와 같이 기존에는 0이 많은 sparse 한 행렬에서 dense 한 행렬로 변환될 수 있다. (*dense 한 행렬을 이용하여 모델링 할 경우 robust한 모형이 나올 수 있다.*)
하지만, SVD의 단점도 존재한다. 계산 비용이 너무 크게 들기도 하며(매트릭스 크기가 커짐에 따라), 새로운 단어나 문서가 존재하면 새롭게 매르틱스를 구한 후, SVD를 해야한다는 점을 들 수 있다.

###### word2vec 기초
자~ 이제 위의 내용을 바탕으로 진짜 이번 강의에서 다루고자 한 word2vec 에 관한 내용을 정리해보자. 이 방법은 기존의 단순한 동시발생(cooccurrence)를 카운팅하는것이 아니라 특정 크기의 window를 기반으로 주변 단어를 예측하고자 하는 방법이다. 우선 방식은 아래의 그림과 같다. 예를들어 window의 길이가 2라고 가정하자. 그러면 아래와 같이 나타낼 수 있다. 검은 줄을 하나하나의 단어라고 하면 파란색의 window와 같이 중심단어를 기준으로 좌우 2개씩 예측을 하고, window가 한칸 넘어가서 빨간색의 window와 같이 예측하고, 이런식으로 문서의 끝까지 진행되는 것을 의미한다.
![word2vec](downloads/lec2/word2vec.png)
여기서 적용되는 목적함수(objective function)는 중심 단어가 주어졌을 때 context 단어(*정확한건지는 모르겠으나 개인적인 이해로는 window 내의 다른 단어가 context 단어라고 생각된다*)가 나올 로그 확률을 최대화 하는 함수를 목적함수로 가진다. 여기서 $\theta$ 는 우리가 최적화 하려고하는 모든 변수들을 나타낸다.
![word2vecobjfun](downloads/lec2/word2vecobjfun.png)

여기에서
$p(w_{t+j} | w_t)$
는 다음과 같이 바꿔서 쓸 수 있다.

$$
p(o | c) = \frac{exp(u_o^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)} --- (1)
$$

여기서 o는 outside(or output) 단어의 index(id) 이며, c는 중심단어의 index를 나타낸다. 또한 $u$와 $v$는 각각 outside 단어의 중심단어 벡터와 중심단어의 outside 벡터를 나타낸다..(*이부분은 명확히 이해가 되지 않는 부분이다. 더 자세한 벡터에 대한 설명이 없어서 추후 강의를 들어야 이해가 될것으로보인다. 본인이 이해하기엔 u는 예측하고자하는(outside)단어에서의 중심단어를 나타내는 벡터이며, v는 반대로 중심단어에서 예측하고자하는(outside)단어의 벡터를 나타내는 것으로보인다.*)

여기서 중요한것은 각각의 단어는 위와 같이 $u$, $v$ 라는 2개의 관점의 벡터를 가진다는점!
(아래는 강의에 나온 표현을 그대로..)

> 1 vector when we represent it as an outside word -> $u$ <br>
  1 vector as we trying to predict the outside word -> $v$

지금까지 목적함수와 목적함수에 들어가있는 로그 확률의 식을 알아봤다. 이제 우리는 목적함수의 최적화 문제를 풀어야한다. 해당 목적함수는 로그 확률의 최대값을 구하는 것이며, 여기에서 우리는 `gradient` 방식을 사용할 것이다. `gradient` 방식은 대게 목적함수를 최소화(minimize) 할때 `gradient descent` 방식을 많이 사용한다. 수식을 사용하지 않고 직관적으로 설명을 하면, 특정 목적함수의 `gradient`(쉽게 기울기) 를 이용해서 이 기울기의 반대방향으로 계속해서 이동하다보면 극소(이는 local minimum일 수도, global minimum일 수도)가 되는 곳에 도달한다는 알고리즘이다(*gradient descent 방법에 대해 자세히 알고 싶다면 [Gradient Descent  탐색 방법](http://darkpgmr.tistory.com/133)을 참고하기 바란다*). `Gradient` 최적화 방법을 사용하기 위해 우리는 목적함수의 `gradient`를 구할 줄 알아야 한다.
여기서 기본적으로 `chain rule` 을 알아야 하는데 간단하게 언급하면 $y=f(u)$ 이고 $u=g(x)$ 일때 즉, $y = f(g(x))$ 일때,

$$
\begin{equation}
\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}  
\end{equation}
$$

로 나타 낼 수 있는것이다. 이는 고등학교때 합성함수의 미분을 배우면서 지나치며 배워온 지식이니 간단하다!
자 이제 목적함수의 `gradient`를 구해보자. 위에서 정의한 (1) 식에서 $v_c$로 편미분을 한다. $v_c$로 편미분을 하는 것은 위에서 언급한 것과 같이 $v$ 벡터가 우리가 예측하고자 하는 단어의 벡터를 나타내고 있으며 이것이 파라미터인 $\theta$에 depend한 변수이므로 이에 대한 `gradient`를 구하는 것이다.

$$
\frac{\partial}{\partial v_c}(\log exp(u_o^T v_c) - \log \sum_{w=1}^W exp(u_w^T v_c))
$$

$$
= \frac{\partial}{\partial v_c}(u_o^T v_c) - \frac{\partial}{\partial v_c}\color{red}{\log} \color{blue}{(\sum_{w=1}^W exp(u_w^T v_c)})
$$

$$
= (u_o) - \frac{\partial}{\partial v_c}\color{red}{f}\color{blue}{(g(v_c))}
$$

여기서 $u_o$ 를 제외한 오른쪽 부분을 따로 `chain rule`을 이용해서 다음과 같이 구할 수 있다.

$$
\frac{\partial}{\partial v_c}\color{red}{f}\color{blue}{(z)} =
\frac{\partial \color{red}{f}\color{blue}{(z)}}{\partial \color{blue}{z}}
\frac{\partial \color{blue}{g(v_c)}}{\partial v_c} =
\frac{1}{\sum_{w=1}^W exp(u_w^T v_c)} \frac{\partial}{\partial v_c}{(\sum_{x=1}^W exp(u_x^T v_c)})
$$


$$
= \frac{1}{\sum_{w=1}^W exp(u_w^T v_c)} {\sum_{x=1}^W exp(u_x^T v_c)} \frac{\partial}{\partial v_c}(u_x^T v_c)
= \frac{\sum_{x=1}^W exp(u_x^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)} (u_x)
$$

위의 식을 최종적으로 정리하면 아래와 같이 간략하게 나타낼 수 있다.

$$
u_o - \sum_{x=1}^W \frac{exp(u_x^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)}  u_x = u_o - \sum_{x=1}^W p(x | c) u_x
$$

최종적으로 목적함수가 위와 같이 `gradient function` 으로 나타났다. 여기서 $W$ 는 전체 단어의 수로 수백만개까지 갈 수 있어서 강의에서는 `very expensive function to compute` 이라 얘기한다. 그래서 이는 approximation 과 normalization 이 필요하다. (*이는 과제에서 다뤄질것으로 보인다..*)


###### Summary
 2강에서는 단어를 어떻게 벡터의 형태로 나타낼 수 있는지에 대해서 간략하게 알아봤다. 벡터로 나타내는 방식에는 `window based cooccurence matrix` 가 있으며, 이 매트릭스를 이용해 SVD를 진행하여 기하학적 의미도 살펴봤다. 그리고 `word2vec` 알고리즘을 진행하기 위해 목적함수와 확률식, 최적화를 위한 `gradient` 도출까지 진행해보았다. 이러한 기초 지식을 가지고 다음강에서부터는 word2vec를 통해 나타난 결과의 의미와, `GloVe` 와 같은 다른 형태의 word vector 표현 방식을 알아보도록 하겠다. 끝~


 $x$에 대한 이차 방정식 $ax^2 + bx + c = 0$의 해는 다음과 같다.
 \\[
 x = \frac{-b \pm \sqrt{b^2-4ac}}{2a}
 \\]


[lec-2]: https://www.youtube.com/watch?v=aRqn8t1hLxs&index=2&list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG&t=3327s
