<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>joongkyunKim.github.io</title>
   
   <link>http://joongkyunKim.github.io</link>
   <description>Postings about what i learned and what i have to learn, especially about Deep Learning</description>
   <language>en-uk</language>
   <managingEditor> joongkyunKim</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>[CS224n] Lecture4 : Word Window Classification and Neural Network</title>
	  <link>//cs224n-lecture4</link>
	  <author>joongkyunKim</author>
	  <pubDate>2017-04-17T00:00:00+09:00</pubDate>
	  <guid>//cs224n-lecture4</guid>
	  <description><![CDATA[
	     <p>이번 포스팅에서는 분류문제란 어떤형태의 문제인지, 그리고 NLP에서의 분류 방법인 window Classification 문제와 Neural Network 에 대해서 간략하게 알아보도록 하자. 수식적인 부분이 많이 나오는 만큼 긴장하길 바란다..</p>

<ul>
  <li>Lecture slide : <a href="http://joongkyunKim.github.io/downloads/lec4/LectureSlide4.pdf">LectureSlide4</a></li>
  <li>Lecture note  : <a href="http://joongkyunKim.github.io/downloads/lec4/LectureNote3.pdf">Lecturenote3</a></li>
  <li>Video Clip    : <a href="https://www.youtube.com/watch?v=uc2_iwVqrRI&amp;list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&amp;index=4">3강</a></li>
</ul>

<h6 id="classification-background">Classification background</h6>
<p>일반적으로 머신러닝에서 분류문제는 {x,y}로 데이터가 주어졌을때, $x$(input) 을 이용해서 $y$ 인 label을 예측하는 것을 의미한다. NLP에서는 $x$ 는 단어 벡터, context windows, 문장 등등이 될 수 있고, $y$는 named entities, 다른 단어, 문법적요소 등이 될 수 있다. 아래의 그림을 보자.</p>

<p><img src="downloads/lec4/classification1.png" alt="classification1" />
위의 문제에서는 2차원에서 녹색점과 빨간점을 구분하는 로지스틱 회귀식을 찾고 싶다. 보통의 머신러닝에서는 x가 고정되어 있다고 가정하고, 로지스틱 회귀분석의 가중치($W$)를 트레이닝한다.
우리가 앞서 배운 내용에서는 위와 같이 적용하기 위해서는 우리는 아래의 식을 이용하여 마찬가지로 가중치 역할을 하는 $W$를 트레이닝을 해야한다.</p>

<p>\begin{equation}
p(y | x) = \frac{exp(W_y. x)}{\sum_{c=1}^C exp(W_c. x)}
\end{equation}</p>

<script type="math/tex; mode=display">( W \in \mathbb R^{C * d} )</script>

<p>위의 1번 식은 두 개의 식으로 쪼개서 아래와 같이 볼 수 있다. 분자 부분은 행렬 연산이며, 이를 표준화(normalize) 시키는 두개의 과정으로 볼 수 있다.</p>

<script type="math/tex; mode=display">W_y.x = \sum_{i=1}^d W_{yi}x_i = f_y</script>

<script type="math/tex; mode=display">p(y | x) = \frac{exp(f_y)}{\sum_{c=1}^C exp(f_c)} = softmax(f_y)</script>

<p>그리고 위의 수식을 트레이닝 하는 과정은 정확히 분류하는 확률을 최대화 하는 방향으로 이뤄질 것이다. 이는 다시말하면 아래의 log 확률을 최소화 하는 과정과 동일하다.</p>

<p>\begin{equation}
-\log p(y | x) = -\log(\frac{exp(f_y)}{\sum_{c=1}^C exp(f_c)})
\end{equation}</p>

<p>위의 2번 수식을 잘 살펴보면, 이는 Cross-Entropy(이하 CE) 함수와 동일하다는 것을 알 수 있다. CE 함수는 아래 수식 3 과 같다. 여기에서 $q(c)$는 앞에서 구한 1번 수식과 동일하고, $p(c)$ 를 $p = [0,…0,1,0,…0]$ 의 형태로 정확한 분류가 이뤄진 곳에서 1, 아닌곳에서 0이란 값을 갖는 확률 분포라고 가정한다면 위의 2번 수식과 CE는 동일한 수식이라고 생각 할 수 있다.</p>

<p>\begin{equation}
H(p,q) = -\sum_{c=1}^C p(c)\log q(c)
\end{equation}</p>

<p>최종적으로 전체 데이터 셋에 대한 CE loss function은 다음과 같다.</p>

<p>\begin{equation}
J(\theta) =\frac{1}{N} \sum_{i=1}^N -\log(\frac{exp(f_{y_i})}{\sum_{c=1}^C exp(f_c)}) + \lambda \sum_k \theta_k^2
\end{equation}</p>

<p>뒤에 추가된 부분은 일반적으로 트레이닝을 할때 overfitting 을 막기위해 추가되는 regularization factor 이다. 일반적으로 딥러닝모델에서는 트레이닝 시간 또는 파라미터의 수가 많을 경우 시간에 따라 training error는 줄어들게 된다. 하지만 이것이 만약에 특정 트레이닝 데이터에 overfitting 되게 되면 test error는 다시 증가하게 된다. 이 점을 방지하기 위해 위와 같이 regularization factor를 추가하게 된다.
그리고 이제부터는 수식을 행렬형태로 좀더 간편하게 나타내기 위해</p>

<script type="math/tex; mode=display">f = Wx</script>

<p>로 표현하기로 하자.</p>

<h6 id="window-classification">Window Classification</h6>
<p>단어를 분류할 때 어려운 점은 그 단어의 ambiguity 때문이다. 같은 단어라도 다른 의미를 지닐때가 있기 때문에 이러한 점을 극복하기 위해서는 그 단어 주위에 있는 context, 즉 문맥을 살펴봐야할 필요가 있다. 예를들어 아래와 같은 경우는 named entities가 불분명한 경우들 이다</p>

<blockquote>
  <p>Paris, France vs. Paris Hilton
Berkshire Hathaway vs. Anne Hathaway</p>
</blockquote>

<p>이러한 모호한 단어들을 제대로 구분하기 위해서 도입된 방법이 <code class="highlighter-rouge">window classification</code>  이다. 위에서 말한 context 를 이용하여 각 단어를 분류하고자 하는 아이디어에서 출발해서 그 범위를 특정 크기의 window에 한정해서 예측하겠다는 내용이다.
예를 들어 이 방식을 이용해서 named entity를 구분할 수 있는데 <code class="highlighter-rouge">사람</code>, <code class="highlighter-rouge">장소</code>, <code class="highlighter-rouge">기관</code> 과 같이 분류할 수 있다.
<code class="highlighter-rouge">Window classification</code> 의 목적은 최종적으로 <code class="highlighter-rouge">window</code> 내의 중심단어를 분류하고자 하는 것이며, 주변 단어를 이용하여 중심단어를 분류하는 softmax classifier를 만드는 것이다.
아래의 예제를 살펴보자.</p>

<p><img src="downloads/lec4/window.png" alt="window" />
여기에서는 중심단어인 <code class="highlighter-rouge">paris</code>를 분류하고자 하는것인데 window 길이가 2이므로 좌우 2개 주변 단어를 이용하여 <code class="highlighter-rouge">paris</code> 를 분류한다. 이 모델의 input으로 들어가는 벡터인 $x_{window}$는 총 5개의 단어를 합친(concatenate) $\in \Bbb R^{5d}$ 에 속한다.
우리는 이 $x_{window}$ 를 위의 (1)번식에 넣어서 확률값을 구할 수 있다. 여기서 나오는 확률은 각각 <code class="highlighter-rouge">사람</code>, <code class="highlighter-rouge">장소</code>, <code class="highlighter-rouge">기관</code> 일 확률이 될 것이다. 그리고 마찬가지로 (4)번식에도 대입하면 error도 구할 수 있다. 하지만 여기서 각 word vector는 어떻게 업데이트를 하는 것일까?</p>

<p>우리는 앞에서 목적함수를 미분을 통해 gradient descent 방식을 진행하는 것을 배웠었다. 이를 이용하는것이다. 즉, 가지고 있는 loss function을 미분하여 업데이트를 진행 할 수 있다. 여기서 주요하게 적용되는 개념이 <code class="highlighter-rouge">chain rule</code> 이며 이는 2강에서 자세하게 다뤘다.</p>

<p>자, 업데이트를 진행하기에 앞서 우선 필요한 notation부터 정의를 한다.</p>

<ul>
  <li>$\hat y$ : softmax 확률의 output vector</li>
  <li>$t$ : 결과값(target) 확률 분포(실제 정답인 class 에서만 1이고, 나머지에선 0을 갖음)</li>
  <li>$f = f(x) = Wx \in \Bbb R^C$ and $f_c$ = f 벡터의 c 번째 원소</li>
</ul>

<p>우선 목적함수 부분에서 $\log softmax(f_y(x))$ 부분에 대해서만 $x$에 depend 하므로 이부분을 미분한다.</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial x} -\log softmax(f_y(x)) = \sum_{c=1}^C - \frac{\partial \log softmax(f_y(x))}{\partial f_c} \frac{\partial f_c(x)}{\partial x}</script>

<p>여기서 $f_c$가 $c=y$(올바른 class) 일때와 $c\neq y$ 일때를 나눠 생각해보면, 다음과 앞부분의 식을 바꿔서 생각할 수 있다.</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial f} -\log softmax(f_y(x)) =  
\begin{bmatrix}
\hat y_1 \\
... \\
\hat y_y -1 \\
... \\
\hat y_C
\end{bmatrix}</script>

<p>여기서 c = y 인 부분만 -1 을 해준다. 이는 다음과 같이 쓸 수 있다.</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial f} -\log softmax(f_y(x)) =  
\begin{bmatrix}
\hat y - t
\end{bmatrix}
 = \delta</script>

<p>t 라는 것이 위에서 정의한대로 실제 클래스이면 1 아니면 0을 나타내는 확률 분포이므로, 위의 두 식은 동일하게 받아들일 수 있다.</p>

<p>최종적으로 위의 업데이트 식은 아래와 같이 쓸 수 있다.</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial x} -\log softmax(f_y(x)) = \sum_{c=1}^C - \frac{\partial \log softmax(f_y(x))}{\partial f_c} \frac{\partial f_c(x)}{\partial x} = \sum_{c=1}^C \delta_c W_{c.}^T</script>

<p>이며 이는 앞에서 한것처럼 간단하게 행렬로 나타낼 수 있다. $W^T\delta$ 로..
최종 업데이트 식은 아래와 같이 쓸 수 있다.</p>

<script type="math/tex; mode=display">\nabla_x J = W^T\delta \in \Bbb R^{5d}</script>

<p><em>위의 과정에서 행렬 연산이 많이 나오는데, 이중에서 $f=Wx$ 계산과 exponential 계산은 여기 표현을 빌리면 expensive 한 계산이다. 이를 코딩할 때의 팁은 모든 word vector를 다 합쳐서 하나의 매트릭스로 만들어서 곱 연산을 하는 것 보다 word vector를 리스트로 만들어서 for loop를 이용해서 반복계산하는것이 시간적으로 효율적이다.</em></p>

<h6 id="neural-network">Neural Network</h6>
<p>지금까지 softmax(=logistic regression) 을 이용한 모델 트레이닝을 진행하였다. 하지만 softmax의 경우, 단지 linear decision boundary를 제시할 뿐이다. 이는 분류 문제에서 상당한 제한으로 적용되며, non-linear 한 boundary 를 위해 neural network 가 제안되었다.</p>

<p>Neural Network 는 많은 사람들이 요즘 핫한 딥러닝의 주요 알고리즘이므로 많이 들어보고 알고 있을것이라 생각되지만, 아주 간략하게 강의 슬라이드 그림으로 설명하고 넘어가겠다.</p>

<p><img src="downloads/lec4/neuron.png" alt="neuron" />
<img src="downloads/lec4/neuralnetwork.png" alt="neuralnetwork" /></p>

<p>위의 사진을 보면, 기본적인 구조가 나타난다. 동그란부분을 뉴런이라고 하며, 왼쪽에서 들어오는 위의 3개의 화살표가 input(x) 이며, 맨 아래 화살표가 bias라는 것을 나타낸다. 이들이 들어오면 선형결합을 통해서 $w^Tx+b$ 가 되며, $f$라는 함수(sigmoid, 지난 3강에서 살펴보았다.) 를 거쳐 output이 나오는 형태이다.
이를 좀 더 복잡한 형태로 해놓은 것이 아래의 그림이며, 우리가 이제부터 직접적으로 사용할 수식은 중간에 존재하는 matrix notation 부분이다!</p>

<p>다시한번 위에서 언급한 $X_{window}$ 를 떠올려보자. 그리고 이 문제를 아까는 softmax에 적용했지만 이번엔 neural network 에 적용해보자. 여기서 우리가 분류하고싶은 문제는 중심 단어가 위치이냐 아니냐에 대한 분류문제이다.
아주 간단히 아래와 같이 3-layer neural network를 생각해보자.</p>

<p><img src="downloads/lec4/score.png" alt="score" />
여기서 $U$라는 것은 중간에 두번째 layer에서 나온 output을 어떠한 score로 변환해주는 가중치라고 생각하면 될것 같다. 우리가 여기서 구하고자 하는 최종 s = score(museums in Paris are amazing) 이며, 이 score는 중심단어인 <code class="highlighter-rouge">paris</code>가 위치인지 아닌지에 대한 score 값이다.</p>

<p>자, 이제 업데이트를 하기 위해서는 목적함수가 있어야한다. 여기에서는 새로운 형태의 목적함수를 제시하는데 <code class="highlighter-rouge">Max-margin loss function</code> 이다. 그 수식은 아래와 같다</p>

<p>\begin{equation}
J = \max (0, 1-s+s_c)
\end{equation}</p>

<p>여기서 $s$ = score(museums in Paris are amazing), $s_c$ = score(Not all museums in Paris) 이며, 위의 수식이 의미하는 바는 진짜 window의 score는 크게, 진짜가 아닌(corrupted) window 의 score는 낮게 하는 것이 목적이다. 그리고 이 목적함수는 continuous한 함수이므로 Stochastic Gradient Descent(SGD) 방식을 이용해 최적화를 진행 할 수 있다.
좀 더 직관적으로 왜 이런 목적함수를 사용했는지 알아보자. 아래의 그림을 살펴보자
<img src="downloads/lec4/margin.png" alt="margin" />
기존의 softmax의 경우 검은 트레이닝 데이터가 존재할때 이를 양분하는 boundary는 빨간 선과 같이 생길수 있다. 이럴 경우 유사한 위치의 빨간 테스트 데이터가 들어왔을 때 분류 정확도가 현저하게 낮아진다. 하지만 오른쪽과 같이 max-margin을 사용하게 되면 boundary가 파란선과 같이 생기며, 비슷한 위치의 빨간 테스트 데이터에 대해서도 높은 성능을 보일 수 있다. 일반적으로 max-margin이 softmax에 비해 more robust 하다고 한다.</p>

<p>위의 목적함수를 전체 데이터에 대해서 진행하는 방식은 앞의 강의에서 언급한 skip-gram에서의 negative sampling 과 유사하다. 즉, true 한 window에 대해서 여러개의 corrupt window를 샘플링해서 위의 목적함수를 구하고, 이를 모든 트레이닝 window에 대해 더하는 형태로 구하게 된다. (<em>이러한 negative sampling을 진행하는 이유는 전체에 대해서 하게 되면 너무 계산의 cost가 크기 때문이다.</em>)</p>

<p>이제 SGD 방식을 적용하기위해 목적함수를 각각 변수인 $U$,$W$,$b$,$x$에 대해 미분한 식을 찾아야한다. 이 과정이 상당히 길기 때문에 포스팅에 직접 싣지는 않겠다. 이 내용은 강의 slide 50쪽부터 58쪽까지 <code class="highlighter-rouge">Training with Backpropagation</code> 파트에 자세하게 설명이 되어있으며, 위에서 우리가 진행한것처럼 <code class="highlighter-rouge">chain rule</code>을 활용하면 충분히 이해할 수 있는 과정이다. 꼭 다음 강의를 위해서 눈으로 따라가며 이해해보기 바란다.</p>

<h6 id="summary">Summary</h6>
<p>이번 4강에서는 일반적인 분류문제와 softmax, Cross-Entropy loss function에 대해서 알아봤다. 또한 특정 단어 분류를 위해 주변 문맥을 이용하는 개념인 window classification 모델과 이를 어떻게 앞에서 배운 softmax에 적용해서 최적화를 진행하는지, 그 미분과정을 자세히 들여다봤다. 그리고 softmax의 linear decision boundary 성질의 한계를 극복하기 위해 non-linear한 Neural Network 에 대해서 간략하게 알아봤으며, 새로운 loss function인 Max-margin loss function 의 쓰임새도 알아보았다. 비록 neural network 에서 SGD를 위한 미분을 다 살펴보진 않았지만 앞서 진행된 방식대로 슬라이드를 보고 따라간다면 충분히 해낼 수 있을 것이다.
다음 강의에서는 Neural network 의 파라미터 업데이트 방식인 backpropagation 방식을 알아보고 좀 더 복잡한 모델을 살펴볼 예정이다. 끝~</p>


	  ]]></description>
	</item>

	<item>
	  <title>[CS224n] Lecture3 : GloVe: Global Vectors for Word Representation</title>
	  <link>//cs224n-lecture3</link>
	  <author>joongkyunKim</author>
	  <pubDate>2017-04-06T00:00:00+09:00</pubDate>
	  <guid>//cs224n-lecture3</guid>
	  <description><![CDATA[
	     <p><em>이번 강의부터는 최근 2017년 CS224d(2016)와 동일한 강좌인 CS224n 강의 비디오가 youtube에 공개되면서 좀 더 따끈따끈한 최신 내용을 반영하고자 CS224n 강의를 정리하도록 하겠다. 앞서 진행된 두 강좌에 대해서는 내용이 거의 비슷하고 순서정도만 변경되어있기 때문에 이어서 진행해도 큰 어려움이 없을것으로 본다</em></p>

<p>이번 3강에서는 앞에서 배운 Word2vec과 이어서 GloVe란 알고리즘에 대해 알아보며, evaluation에 대해서 알아보도록 한다.
새롭게 CS224n 강의에 대해서 진행하기 때문에 기존의 CS224d 강의 youtube 링크와 다르다는 것을 주의하기 바란다!</p>

<ul>
  <li>Lecture slide : <a href="http://joongkyunKim.github.io/downloads/lec3/LectureSlide3.pdf">LectureSlide3</a></li>
  <li>Lecture note  : <a href="http://joongkyunKim.github.io/downloads/lec3/LectureNote2.pdf">Lecturenote2</a></li>
  <li>Video Clip    : <a href="https://www.youtube.com/watch?v=ASn7ExxLZws&amp;list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&amp;index=3&amp;t=672s">3강</a></li>
</ul>

<h6 id="lecture-2-review">Lecture 2 Review</h6>
<p>3강에 대한 내용을 언급하기 이전에 2강에서 진행된 Word2vec 내용과 Gradient 에 대해서 다시한번 상기시키도록 하겠다. (<em>2강은 cs224d 강의내용이었기 때문에 cs224n으로 바뀌면서 언급하지 못한 내용에 대해서도 간략하게 언급하도록 하겠다</em>)</p>

<p>지난 강의에서 다뤘던 것중에 가장 중요한 내용은 Word2vec에서 어떻게 벡터를 이용해서 단어를 예측하는지, 확률의 정의는 어떻게 되는지, 그리고 목적함수가 어떠한 형태인지, 이를 최적화하기위해 gradient를 어떻게 구하는지에 대해서 쭉 배워보았다. (<a href="https://joongkyunkim.github.io/cs224d-lecture2">지난 강의</a>)
지난 강의에서 중심단어를 놓고, 좌우 window 크기만큼 예측을 하고, 한칸씩 이동을하면서 전체 문서의 단어들을 예측하는 그림을 기억할 것이다. 이는 <code class="highlighter-rouge">Skip-grams</code> 이라는 알고리즘이다. 이는 <code class="highlighter-rouge">Skip-grams</code>는 다음과 같이 설명된다.
&gt; Predict context	words	given	target	(position	independent)</p>

<p>그리고 각 단어를 예측할 확률과 최종 목적함수는 다음과 같다.</p>

<p>\begin{equation}
p(o | c) = \frac{exp(u_o^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)}
\end{equation}</p>

<p>\begin{equation}
J(\theta) = \frac{1}{T} \sum_{t=1}^T\sum_{-m \le j \le m, j\neq 0} \log p(w_{t+j}|w_t)
\end{equation}</p>

<p>여기서 o는 outside(or output) 단어의 index(id) 이며, c는 중심단어의 index를 나타낸다. 또한 $u$와 $v$는 각각 outside 단어의 중심단어 벡터와 중심단어의 outside 벡터를 나타낸다. 즉, 각 단어는 두개의 벡터를 가지고 있음을 알 수 있다.</p>

<p>위의 목적함수를 최적화 하는 과정에서 update를 하는데 <code class="highlighter-rouge">gradient descent</code> 방식을 쓴다고 언급했었다. 하지만 이 방식은 document 내의 모든 단어에 대해 위의 확률을 다 계산한 다음 update하는 방식을 취한다. 하지만 이 경우에는 단어의 수가 매우 많을 경우 아주 비효율적인 update방식이므로 이에 대한 대책으로 <code class="highlighter-rouge">stochastic gradient descent</code> 방법이다. 이는 파라미터인 $\theta$ 를 각각의 window 를 진행한다음 지속적으로 upgrade 하겠다는 방식이다.</p>

<p><img src="downloads/lec3/stochasticgd.png" alt="stochasticgd" /></p>

<h6 id="skip-gram-model--negative-sampling">Skip-gram model &amp; negative sampling</h6>
<p>이제부터는 3강에서 추가된 내용에 대해 정리해볼 시간이다. (1)번 식에서 우리는 분모에 해당하는 부분을 계산하는데 상당히 많은 시간과 비용이 들것이라고 쉽게 예상할 수 있다.(<em>모든 단어에 대해서 두 벡터의 내적을 계산해야하므로</em>)
그래서 이 강의에서는 <code class="highlighter-rouge">negative sampling</code> 이라는 방식을 추천한다. 여기서 <code class="highlighter-rouge">negative sampling</code> 이라는 방식은 실제 center word와 context word가 아닌 random word를 일부 선택하여 binary 로지스틱 모델링을 하는 방식이다. 아래의 새롭게 제시된 목적함수를 살펴보자</p>

<p>(paper from “Distributed Representations of Words and Phrases and their Compositionality”(Mikolov et al.2013))</p>

<p>\begin{equation}
J_{t}(\theta) = \log \sigma(u_{o}^T v_{c}) + \sum_{i}^k E_{j \sim P(w)}[\log \sigma(-u_{o}^T v_{c})]
\end{equation}</p>

<p>\begin{equation}
J(\theta) = \frac{1}{T}\sum_{t=1}^T J_{t}(\theta)
\end{equation}</p>

<p>여기서 $\sigma$ 는 일반적으로 우리가아는 시그모이드 함수를 나타낸다($ \sigma(x) = \frac{1}{1+e^{-x}}$)
위의 식을 살펴보면 시그모이드 함수가 로지스틱 함수이므로 위와같이 로지스틱 모델링이란 말이 성립이된다. 또한 뒷부분은 $P(w)$ 의 확률분포를 따르는 j라는 샘플의 시그모이드 함수의 기대값을 나타낸다.
위의 (3)번 식은 아래와 같이 좀 더 명확하게 나타낼 수 있다.</p>

<p>\begin{equation}
J_{t}(\theta) = \log \sigma(u_{o}^T v_{c}) + \sum_{j \sim P(w)}[\log \sigma(-u_{o}^T v_{c})]
\end{equation}</p>

<p>이며, $P(w) = U(w)^{3/4} / Z$ 로 random negative sampling 을 하는 확률분포는 Uniform 분포를 변형하여 만들어진 분포를 따른다.
위의 식을 해석하면 실제 center word와 context word 가 나올 확률자체는 maximize 시키고, 뒤에 있는 random word 에 따른 확률은 minimize 할려는 목적함수를 의미한다. (<em>참고 : $\sigma(-x) = 1-\sigma(x)</em>)
우리는 위의 과정을 통해서, 실제 (2)번 목적함수 계산을 위해 계산비용이 매우 높은 (1)식의 확률을 계산해야했지만, 아래와 같이 <code class="highlighter-rouge">negative sampling</code> 방식을 이용함으로써 일정 k개의 단어들에 대해서만 계산을 해주면된다. 일반적으로 k는 강의에서는 10개를 사용한다고 한다.</p>

<p>추가로, 앞서 설명한 <code class="highlighter-rouge">Skip-gram</code> 모델 이외에 제시된 모델은 <code class="highlighter-rouge">Continuous bag of words(CBOW)</code> 라는 모델로, <code class="highlighter-rouge">Skip-gram</code> 은 center word로 부터 주변의 각각의 단어들을 예측하는 모델이라면, <code class="highlighter-rouge">CBOW</code> 모델은 주변 단어들의 vector를 이용하여 center word를 예측하는 모델이다.(<em>이에 대한 설명은 따로 진행되진 않고 Assignment 에서 다뤄지는것으로 보인다</em>)</p>

<p>이렇게 <code class="highlighter-rouge">Skip-gram</code>을 이용하여 단어를 벡터로 나타내는 word2vec 방식은 단어들의 동시발생 그자체에 관심이 있는데, 동시발생 <code class="highlighter-rouge">빈도</code>에 관심을 가지는 방식이 앞서 2강에서 설명한 <code class="highlighter-rouge">window based co-occurence matrix</code> 를 이용하는 방식이다(<em>CS224n 에서는 이 뒤에 2강에서 언급한 window based Co-Occurrence matrix를 설명한다. 이는 2강에서 자세히 다뤘기때문에 생략하기로 한다</em>)</p>

<h6 id="glove--global-vectors-for-word-representation">GLoVe : Global Vectors for Word Representation</h6>
<p>이번에는 GLoVe 라는 알고리즘에 대해 알아보자. GLoVe는 앞서 2강에서 언급한 <code class="highlighter-rouge">window based co-occurence matrix</code> 를 이용한다. 아래 GLoVe의 목적함수 식을 보자.</p>

<p>\begin{equation}
J(\theta) = \frac{1}{2} \sum_{i,j=1}^W f(P_{ij})(u_i^Tv_j - \log P_{ij})
\end{equation}</p>

<script type="math/tex; mode=display">% <![CDATA[
f(X_{ij})=
\begin{cases}
(\frac{X_{ij}}{x_{max}})^{\alpha} & \text{if $X_{ij} \le x_{max}$} \\
1 &\text{otherwise}
\end{cases} %]]></script>

<p>여기에서 $P_{ij}$는 co-occurence matrix에서의 i,j번째 원소를 말하며, GLoVe 모형은 위의 목적함수를 최대화하는 u,v 벡터를 찾고자한다. 이 모형은 트레이닝이 빠르고, 작은 corpus에서도 좋은 성능을 나타내서 많이 이용하는 알고리즘 중 하나이다.
이 GLoVe 모형은 <code class="highlighter-rouge">비유(Word Vector Analogy)</code> 에서 좋은 성능을 나타낸다. 예를 들어 man:woman :: king: ? 라고 되어있다면, 물음표자리에는 대부분의 사람들이 queen을 유추해낼 수 있을 것이다. 이러한 비유의 관계를 GLoVe 모형이 잘 나타낸다. 그리고 이러한 비유하는 것을 알아내는 식은 아래와 같이 코사인 유사도를 이용하여 제일 높은 유사도를 갖는 d를 찾는 것이다. (a:b :: c: ?)</p>

<p>\begin{equation}
d = arg \max_i \frac{(x_b-x_a+x_c)^T x_i}{ \Vert x_b-x_a+x_c \Vert}
\end{equation}</p>

<p>이와 같은 방식으로 아래와 같이 대응되는 단어의 쌍, Company-CEO, 원형-비교급-최상급과 같은 연결을 찾을 수 있다.(그림상에서 두 점(단어)를 잇는 점선의 방향들이 유사함을 알 수 있다). 또한 이러한 비유, 유추는 문법적인(syntactic) 관계와 의미적인(Semantic) 관계 유추에도 활용될 수 있다.</p>

<p><img src="downloads/lec3/glovevis.png" alt="glovevis" />
<img src="downloads/lec3/gloveceo.png" alt="gloveceo" />
<img src="downloads/lec3/glovesuper.png" alt="glovesuper" /></p>

<p>그리고 GLoVe 모델은 다른 여러 모델과 비교하여 정확도 측면에서도 좋은 성능을 보여준다. 아래에서 보듯이 training 데이터 사이즈가 커질수록 당연하게 정확도가 높아짐을 알 수 있고, 맨아래의 모형 파라미터에서는 GLoVe가 300차원의 42billion 단어수를 트레이닝했을때 정확도가 상당히 높음을 알 수 있다. 또한 이전에 배운 Skip-gram 모형보다도 트레이닝 속도가 더 빠른 결과를 보여준다.</p>

<p><img src="downloads/lec3/gloveaccuracy.png" alt="gloveaccuracy" />
<img src="downloads/lec3/glovetrainingtime.png" alt="glovetrainingtime" /></p>

<h6 id="summary">Summary</h6>
<p>새롭게 시작하는 CS224n 3강에서는 앞서 2강에서 배운 Word2vec Skip-gram 모형과 Count base 인 Window based co-occurence matrix를 이용한 GLoVe 모형에 대해서 알아봤다. Skip-gram 모형에서 계산 비용을 줄이기 위한 negative sampling도 언급이 되었고, GLoVe모형이 어떤 목적함수를 갖고, 이것이 실제로 어떠한 장점을 갖는 모형인지, 그리고 다른 모델과 비교하는 알고리즘의 장점에 대해서도 알아봤다. 다음 강의에서는 Word Classification과 Neural Network에 대해서 좀 더 알아보기로 하자. 끝~</p>


	  ]]></description>
	</item>

	<item>
	  <title>[CS224d] Lecture2 : Word2vec</title>
	  <link>//cs224d-lecture2</link>
	  <author>joongkyunKim</author>
	  <pubDate>2017-03-27T00:00:00+09:00</pubDate>
	  <guid>//cs224d-lecture2</guid>
	  <description><![CDATA[
	     <p>이번 포스팅에서는 lecture1:introduction 에 이어서 lecture2: Word vectors에 관해서 내용을 정리해보도록 한다. 우선 지난 시간에 했던 내용을 잠시 상기시켜보면, Natural Language Processing(이하 NLP)와 Deep Learning에 대해서 개략적인 내용을 살펴봤으며, 딥러닝을 NLP에 적용했을 때 기존의 해결 방식과 차이점을 비교해 보았다. 여기서 결정적으로 중요했던 부분은 딥러닝에 NLP를 적용하기 위해서는 글자 하나하나를 벡터의 형태로 나타내야 한다는 공통점을 보였다. 이번 강의에서는 어떻게 글자를 벡터로 나타내는지에 대한 내용이다.</p>

<ul>
  <li>Lecture slide : <a href="http://joongkyunKim.github.io/downloads/lec2/LectureSlide2.pdf">LectureSlide2</a></li>
  <li>Lecture note(lecture1과 동일)  : <a href="http://joongkyunKim.github.io/downloads/lec1/LectureNote1.pdf">Lecturenote1</a></li>
  <li>Video Clip    : <a href="https://www.youtube.com/watch?v=aRqn8t1hLxs&amp;index=2&amp;list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG&amp;t=3327s">2강</a></li>
</ul>

<h3 id="lecture2--word-vectors">Lecture2 : Word vectors</h3>

<p>컴퓨터가 단어의 의미를 나타내기 위해 활용했던 방법으로 WordNet과 같이 상위-하위어의 관계를 보여주는 분류체계(taxonomy)로 나타내거나 동의어 집합을 보여주는 것이 종종 사용되었다.
<img src="downloads/lec2/wordnet.png" alt="WordNet" />
하지만 위의 방식에는 몇몇 문제점이 존재한다. 먼저,’good’ 의 동의어로 나온 adept, expert 등등은 동의어이지만 문맥에 따라 같이 쓰일 수 없다.(<code class="highlighter-rouge">nuance</code>의 차이) 또한 사람들이 하나하나 수작업으로 매칭작업을 진행해줘야하며 단어간의 유사성 계산에 있어서 힘들 수 밖에 없다.</p>

<p>또한 문장 또는 문서에서 해당 단어가 나오는 부분에 1, 아닌 부분에 0으로 만드는 벡터의 경우(아래그림)는 대부분 0으로 구성된 벡터이다. 그래서 아래의 예제와 같이 <code class="highlighter-rouge">motel</code>과 <code class="highlighter-rouge">hotel</code>은 유사한 단어지만 <code class="highlighter-rouge">AND</code> 연산을 시행했을 때 0으로 아무 연관이 없는것 처럼 표현되기도 한다.
<img src="downloads/lec2/onehot.png" alt="onehot" /></p>

<h6 id="window-based-cooccurence-matrix">Window based cooccurence matrix</h6>

<p>이러한 문제점을 해결하기 위해 제시된 방식이 표현하고자 하는 단어의 이웃(neighbors)를 표현하는 방식이다. 이를 이용해서 표현한 단어들의 벡터의 집합을 <code class="highlighter-rouge">window based cooccurence matrix</code> 라 한다. 아래와 같이 3개의 문장이 있다고 가정하자. 해당 예제에서는 window의 길이를 1로 하고, symmetric한 구조를 가진다. 이렇게되면 focus 하고 있는 단어의 좌/우 1개씩을 window로 설정하는 방식이다. 예를들어 <code class="highlighter-rouge">like</code>라는 단어를 보면, <code class="highlighter-rouge">like</code> 좌우 이웃된 단어들은 첫문장에서 <code class="highlighter-rouge">I</code>,<code class="highlighter-rouge">deep</code> 그리고 두번째 문장에서 <code class="highlighter-rouge">I</code>,<code class="highlighter-rouge">NLP</code> 이다. 이 결과가 아래의 매트릭스에서 counting 된것을 확인할 수 있다. 이렇게 계산된 matrix가 아래와 같고 이는 당연스럽게 symmetric한 matrix 일 수 밖에 없다.</p>

<blockquote>
  <p>I like deep learning.<br />
I like NLP.<br />
I enjoy flying.</p>
</blockquote>

<p><img src="downloads/lec2/cooccurence.png" alt="cooccurence" /></p>

<p>하지만, 위의 방식에는 큰 문제점이 있다! 단어 수에 따라 matrix의 크기가 기하급수적으로 늘어날 뿐 아니라, matrix의 차원(dimension)이 커지면서 계산시에 매우 비효율적일 수 있다. 이러한 문제를 해결하기 위해 <code class="highlighter-rouge">차원축소</code> 기법이 필요하다</p>

<h6 id="singular-value-decompositionsvd">차원축소기법 : Singular Value Decomposition(SVD)</h6>
<p>Singular Value Decomposition(이하 SVD) 는 특잇값 분해라고 말하며, 이는 선형대수에서 고유값 분해(eigenvalue decomposition)과 함께 아주 기본적이고 매우 많이 쓰이는 행렬분해 기법이다. $U$는 $AA^T$를 고유값분해(eigendecomposition)해서 얻어진 직교행렬(orthogonal matrix)로 $U$의 열벡터들을 $A$의 left singular vector라 부른다.또한 $V$는 $A^TA$를 고유값분해해서 얻어진 직교행렬로서 $V$의 열벡터들을 $A$의 right singular vector라 부른다.마지막으로, $\Sigma$는 $AA^T$, $A^TA$를 고유값분해해서 나오는 고유값(eigenvalue)들의 square root를 대각원소로 하는 m x n 직사각 대각행렬로 그 대각원소들을 $A$의 특이값(singular value)이라 부른다. (<em>좀더 자세한 내용은 아래 출처인 ‘다크프로그래머’ 블로그를 참조하기 바란다. SVD에 대해 아주 꼼꼼하고 다방면으로 잘 정리된 블로그이다.</em>)</p>

<p>출처: http://darkpgmr.tistory.com/106 (다크프로그래머)</p>

<p>이제 다시 강의내용으로 돌아와서 아래의 SVD 분해 형태를 살펴보자
<img src="downloads/lec2/SVD.png" alt="SVD" />
위의 케이스는 full SVD형태이지만 잘 쓰지 않는다. 아래와 같이 reduced SVD를 일반적으로 쓰게 되는데, 특잇값중에서 0이 아닌 것들을 모아서 좀 더 압축된 형태로 분해하게 된다. (<em>불필요한(덜 중요한) 정보를 감안하지 않는다고 생각하면 될 것 같다.</em>) 이와 같이 축약된 형태의 SVD는 데이터 압축 등에 종종 사용되니 알아두도록 하자!
최종적으로, 우리도 <code class="highlighter-rouge">window based cooccurence matrix</code> 를 축약된 SVD를 이용해서 차원 축소를 시킬것이다.</p>

<p>이 차원 축소 과정을 아래의 파이썬 코드로 실행해 볼 수 있다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">##python 3.0 이상 버전입니다.</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">la</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s">"I"</span><span class="p">,</span> <span class="s">"like"</span><span class="p">,</span><span class="s">"enjoy"</span><span class="p">,</span><span class="s">"deep"</span><span class="p">,</span><span class="s">"learning"</span><span class="p">,</span><span class="s">"NLP"</span><span class="p">,</span><span class="s">"flying"</span><span class="p">,</span><span class="s">"."</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">la</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">,])</span></code></pre></figure>

<p>이 코드의 결과는 아래와 같이 나온다.
<img src="downloads/lec2/SVDresult.png" alt="SVDresult" /></p>

<script type="math/tex; mode=display">U[0,] =\begin{pmatrix} -0.52412493 \\\ -0.57285914 \\\ 0.0954463 \\\  0.38322849 \\\ -0.17696338 \\\ -0.17609218 \\\ -0.4191856 \\\ -0.05577027\end{pmatrix}</script>

<p>위의 그림을 보면, U행렬을 첫열과 둘째열을 plot으로 나타낸것이다. 기하학적 의미를 간략하게 살펴보면 위에서 구한 <code class="highlighter-rouge">window based cooccurence matrix</code> 에서 단어간의 유사도를 살펴볼 수 있다. 이는 아래 그림의 예제에서 살펴볼 수 있는데, 동일하게 SVD를 통해서 plot한 결과가 같은 단어의 시제끼리 비슷한 위치에 존재하는 것을 알 수 있다.
<img src="downloads/lec2/SVDexp.png" alt="SVDexp" />
출처 : An Improved Model of Seman4c Similarity Based on Lexical Co-Occurrence.Rohde et al. 2005</p>

<p>그리고 이렇게 SVD 과정을 거치게 되면 아래의 벡터와 같이 기존에는 0이 많은 sparse 한 행렬에서 dense 한 행렬로 변환될 수 있다. (<em>dense 한 행렬을 이용하여 모델링 할 경우 robust한 모형이 나올 수 있다.</em>)
하지만, SVD의 단점도 존재한다. 계산 비용이 너무 크게 들기도 하며(매트릭스 크기가 커짐에 따라), 새로운 단어나 문서가 존재하면 새롭게 매르틱스를 구한 후, SVD를 해야한다는 점을 들 수 있다.</p>

<h6 id="word2vec-skip-gram-model">word2vec 기초(Skip-gram model)</h6>
<p>자~ 이제 위의 내용을 바탕으로 진짜 이번 강의에서 다루고자 한 word2vec 에 관한 내용을 정리해보자. 이 방법은 기존의 단순한 동시발생(cooccurrence)를 카운팅하는것이 아니라 특정 크기의 window를 기반으로 주변 단어를 예측하고자 하는 방법이다. 우선 방식은 아래의 그림과 같다. 예를들어 window의 길이가 2라고 가정하자. 그러면 아래와 같이 나타낼 수 있다. 검은 줄을 하나하나의 단어라고 하면 파란색의 window와 같이 중심단어를 기준으로 좌우 2개씩 예측을 하고, window가 한칸 넘어가서 빨간색의 window와 같이 예측하고, 이런식으로 문서의 끝까지 진행되는 것을 의미한다.
<img src="downloads/lec2/word2vec.png" alt="word2vec" />
여기서 적용되는 목적함수(objective function)는 중심 단어가 주어졌을 때 context 단어(<em>정확한건지는 모르겠으나 개인적인 이해로는 window 내의 다른 단어가 context 단어라고 생각된다</em>)가 나올 로그 확률을 최대화 하는 함수를 목적함수로 가진다. 여기서 $\theta$ 는 우리가 최적화 하려고하는 모든 변수들을 나타낸다.
<img src="downloads/lec2/word2vecobjfun.png" alt="word2vecobjfun" /></p>

<p>여기에서
$p(w_{t+j} | w_t)$
는 다음과 같이 바꿔서 쓸 수 있다.</p>

<script type="math/tex; mode=display">p(o | c) = \frac{exp(u_o^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)} --- (1)</script>

<p>여기서 o는 outside(or output) 단어의 index(id) 이며, c는 중심단어의 index를 나타낸다. 또한 $u$와 $v$는 각각 outside 단어의 중심단어 벡터와 중심단어의 outside 벡터를 나타낸다..(<em>이부분은 명확히 이해가 되지 않는 부분이다. 더 자세한 벡터에 대한 설명이 없어서 추후 강의를 들어야 이해가 될것으로보인다. 본인이 이해하기엔 u는 예측하고자하는(outside)단어에서의 중심단어를 나타내는 벡터이며, v는 반대로 중심단어에서 예측하고자하는(outside)단어의 벡터를 나타내는 것으로보인다.</em>)</p>

<p>여기서 중요한것은 각각의 단어는 위와 같이 $u$, $v$ 라는 2개의 관점의 벡터를 가진다는점!
(아래는 강의에 나온 표현을 그대로..)</p>

<blockquote>
  <p>1 vector when we represent it as an outside word -&gt; $u$ <br />
  1 vector as we trying to predict the outside word -&gt; $v$</p>
</blockquote>

<p>지금까지 목적함수와 목적함수에 들어가있는 로그 확률의 식을 알아봤다. 이제 우리는 목적함수의 최적화 문제를 풀어야한다. 해당 목적함수는 로그 확률의 최대값을 구하는 것이며, 여기에서 우리는 <code class="highlighter-rouge">gradient</code> 방식을 사용할 것이다. <code class="highlighter-rouge">gradient</code> 방식은 대게 목적함수를 최소화(minimize) 할때 <code class="highlighter-rouge">gradient descent</code> 방식을 많이 사용한다. 수식을 사용하지 않고 직관적으로 설명을 하면, 특정 목적함수의 <code class="highlighter-rouge">gradient</code>(쉽게 기울기) 를 이용해서 이 기울기의 반대방향으로 계속해서 이동하다보면 극소(이는 local minimum일 수도, global minimum일 수도)가 되는 곳에 도달한다는 알고리즘이다(<em>gradient descent 방법에 대해 자세히 알고 싶다면 <a href="http://darkpgmr.tistory.com/133">Gradient Descent  탐색 방법</a>을 참고하기 바란다</em>). <code class="highlighter-rouge">Gradient</code> 최적화 방법을 사용하기 위해 우리는 목적함수의 <code class="highlighter-rouge">gradient</code>를 구할 줄 알아야 한다.
여기서 기본적으로 <code class="highlighter-rouge">chain rule</code> 을 알아야 하는데 간단하게 언급하면 $y=f(u)$ 이고 $u=g(x)$ 일때 즉, $y = f(g(x))$ 일때,</p>

<script type="math/tex; mode=display">\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}</script>

<p>로 나타 낼 수 있는것이다. 이는 고등학교때 합성함수의 미분을 배우면서 지나치며 배워온 지식이니 간단하다!
자 이제 목적함수의 <code class="highlighter-rouge">gradient</code>를 구해보자. 위에서 정의한 (1) 식에서 $v_c$로 편미분을 한다. $v_c$로 편미분을 하는 것은 위에서 언급한 것과 같이 $v$ 벡터가 우리가 예측하고자 하는 단어의 벡터를 나타내고 있으며 이것이 파라미터인 $\theta$에 depend한 변수이므로 이에 대한 <code class="highlighter-rouge">gradient</code>를 구하는 것이다.</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial v_c}(\log exp(u_o^T v_c) - \log \sum_{w=1}^W exp(u_w^T v_c))</script>

<script type="math/tex; mode=display">= \frac{\partial}{\partial v_c}(u_o^T v_c) - \frac{\partial}{\partial v_c}\color{red}{\log} \color{blue}{(\sum_{w=1}^W exp(u_w^T v_c)})</script>

<script type="math/tex; mode=display">= (u_o) - \frac{\partial}{\partial v_c}\color{red}{f}\color{blue}{(g(v_c))}</script>

<p>여기서 $u_o$ 를 제외한 오른쪽 부분을 따로 <code class="highlighter-rouge">chain rule</code>을 이용해서 다음과 같이 구할 수 있다.</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial v_c}\color{red}{f}\color{blue}{(z)} =
\frac{\partial \color{red}{f}\color{blue}{(z)}}{\partial \color{blue}{z}}
\frac{\partial \color{blue}{g(v_c)}}{\partial v_c} =
\frac{1}{\sum_{w=1}^W exp(u_w^T v_c)} \frac{\partial}{\partial v_c}{(\sum_{x=1}^W exp(u_x^T v_c)})</script>

<script type="math/tex; mode=display">= \frac{1}{\sum_{w=1}^W exp(u_w^T v_c)} {\sum_{x=1}^W exp(u_x^T v_c)} \frac{\partial}{\partial v_c}(u_x^T v_c)
= \frac{\sum_{x=1}^W exp(u_x^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)} (u_x)</script>

<p>위의 식을 최종적으로 정리하면 아래와 같이 간략하게 나타낼 수 있다.</p>

<script type="math/tex; mode=display">u_o - \sum_{x=1}^W \frac{exp(u_x^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)}  u_x = u_o - \sum_{x=1}^W p(x | c) u_x</script>

<p>최종적으로 목적함수가 위와 같이 <code class="highlighter-rouge">gradient function</code> 으로 나타났다. 여기서 $W$ 는 전체 단어의 수로 수백만개까지 갈 수 있어서 강의에서는 <code class="highlighter-rouge">very expensive function to compute</code> 이라 얘기한다. 그래서 이는 approximation 과 normalization 이 필요하다. (<em>이는 과제에서 다뤄질것으로 보인다..</em>)</p>

<h6 id="summary">Summary</h6>
<p>2강에서는 단어를 어떻게 벡터의 형태로 나타낼 수 있는지에 대해서 간략하게 알아봤다. 벡터로 나타내는 방식에는 <code class="highlighter-rouge">window based cooccurence matrix</code> 가 있으며, 이 매트릭스를 이용해 SVD를 진행하여 기하학적 의미도 살펴봤다. 그리고 <code class="highlighter-rouge">word2vec</code> 알고리즘을 진행하기 위해 목적함수와 확률식, 최적화를 위한 <code class="highlighter-rouge">gradient</code> 도출까지 진행해보았다. 이러한 기초 지식을 가지고 다음강에서부터는 word2vec를 통해 나타난 결과의 의미와, <code class="highlighter-rouge">GloVe</code> 와 같은 다른 형태의 word vector 표현 방식을 알아보도록 하겠다. 끝~</p>


	  ]]></description>
	</item>

	<item>
	  <title>[CS224d] Lecture1 : Intro to NLP&Deep Learning</title>
	  <link>//cs224d-lecture1</link>
	  <author>joongkyunKim</author>
	  <pubDate>2017-03-22T21:30:00+09:00</pubDate>
	  <guid>//cs224d-lecture1</guid>
	  <description><![CDATA[
	     <p>Deep Learning 학습을 위해 CS224d 강의를 보고 강의의 복습 및 정리를 위해 오늘부터 Review 내용을 정리하기로 했다. CS224d : Deep Learning for Natrual Language Processing(이하 NLP) 강의는 Stanford University에서 진행되었고 지속해서 진행되고 있는 강의로 해외뿐만아니라 우리나라에서도 딥러닝 초보 학습자들에게 인기를 모은 강의이다. 현재 2017년에는 CS224n 강의가 새롭게 진행중이지만, 강의 비디오를 볼 수 없는 관계로 youtube에서 강의를 볼 수 있는 2016년 강의인 CS224d를 학습하고 정리한다.</p>

<p>오늘은 Lecture1인 Intro to NLP and Deep Learning 에 대해서 알아보기로 한다.</p>

<ul>
  <li>Lecture slide : <a href="http://joongkyunKim.github.io/downloads/lec1/LectureSlide1.pdf">LectureSlide1</a></li>
  <li>Lecture note  : <a href="http://joongkyunKim.github.io/downloads/lec1/LectureNote1.pdf">Lecturenote1</a></li>
  <li>Video Clip    : <a href="https://www.youtube.com/watch?v=Qy0oEkCZkBI&amp;list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG&amp;index=1">1강</a></li>
</ul>

<h3 id="lecture1--introduction">Lecture1 : Introduction</h3>

<p>첫 강좌에서는 기본적인 NLP의 개념, 응용과 딥러닝의 개념 및 응용에 대해서 설명하고있다.</p>

<h6 id="nlpnatural-language-processing">NLP(Natural Language Processing)</h6>
<p>NLP는 크게 computer science, artificial intelligence(AI), linguistics(언어학) 분야가 접합된 것으로, 결국엔 컴퓨터가 자연언어를 ‘이해’ 할 수 있게 처리를 해주는것을 의미한다.
본 수업에서는 주로 Syntactic analysis(구문 분석)과 Semantic analysis(의미, 맥락 분석)에 포커스를 맞춰서 수업이 진행된다.</p>

<p>NLP의 응용분야는 spelling checking, keyword search 등과 같이 매우 단순한 분야부터 글의 긍정/부정 분류, 기계 번역, 질문 응답과 같은 어려운 분야까지 다양하게 응용될 수 있다.</p>

<p>NLP에서의 어려운점은 기계는 우리의 언어를 문맥상으로 파악하기 힘든것에 있다. 아래의 예제를 살펴보자</p>

<blockquote>
  <p><em>Kim hit Park and then she [fell/ran]</em></p>
</blockquote>

<p>해당 예제의 경우,  <code class="highlighter-rouge">fell</code>이 선택될 경우에는  <code class="highlighter-rouge">she</code>가 지칭하는 것이  <code class="highlighter-rouge">Park</code>이 되지만 <code class="highlighter-rouge">ran</code>의 경우에는 <code class="highlighter-rouge">she</code>가 지칭하는것이 <code class="highlighter-rouge">Kim</code>이 된다. 이와 같이 어느 동사가 오느냐에 따라 지칭(refer)하는 것이 달라질 수 있으며, 인간은 이 문장을 문맥상으로 이해하여 찾아낼 수 있지만 컴퓨터는 그렇게 하지 못한다. 또한, 이중적인 문장과 같은 경우도 컴퓨터가 올바른 답을 내놓지 못할 수도 있다.</p>

<h6 id="dldeep-learning">DL(Deep Learning)</h6>
<p>딥러닝은 머신러닝(Machine Learning)의 분야 중 하나로, 요즘 computer vision, speech recognition 등과 같은 분야에서 각광받고 있다. 대부분의 머신러닝 방법은 well defined 된 human-designed representation와 input features에 잘 적용이 되며, 최종 예측을 위해 parameter들을 최적화하는 작업이 주가 된다. 반면, 딥러닝의 경우는 이와 달리 human-designed된 feature도 사용하지 않고, raw한 데이터를 넣어서 features 까지 learning해서 구해버린다.
바로 이점이 딥러닝이 요새 각광받고 있는 이유가 될것이다. 특정 분야에 대하여 features를 만들어서 이를 튜닝하려면 해당 분야의 전문 지식(Domain Knowledge)가 상당히 필요하지만, 딥러닝을 이용한다면 데이터를 통해서 모델이 <strong>learn feature itself</strong> 를 통해 features를 만들어주므로, 손수 힘들게 features를 만들필요가 없는것이다!!
이를 위해서는 상당한 computing power (learning time을 줄이기 위해)가 필요한데, 최근 CPU/GPU의 발전과 함께 물만난 고기마냥 딥러닝이 높은 성능을 보여주고 있다.</p>

<h6 id="deep-learning--nlp--deep-nlp">Deep Learning + NLP = Deep NLP</h6>
<p>representation learning 과 deep learning 방법을 이용해서 NLP 문제들을 더 좋은 성능으로 풀어보기 위해 NLP에 딥러닝이 적용되었다. 이를 통해서 NLP 각 영역에서 좋은 성능 향상을 이뤄냈다.</p>

<ol>
  <li>
    <p><strong>Phonology(음운체계)</strong>
<img src="downloads/lec1/phonemes.png" alt="PHONEMES" />
기존에는 위의 표와 같이 정해진 음운체계표를 만들어서 적용을 했지만, 딥러닝이 적용된 현재에는 이러한 음운을 예측하기위해 sound features를 training하고, 결과를 숫자 벡터로 표현한다.</p>
  </li>
  <li>
    <p><strong>Morphology(형태론)</strong>
<img src="downloads/lec1/morphemes.png" alt="morphemes" />
<img src="downloads/lec1/morphology.png" alt="MORPHOLOGY" />
기존에는 위의 표와 같이 단어를 형태소(접두사/어근/접미사 등)의 형태로 나눠서 구분지었지만, 딥러닝을 적용하면서 아래와 같이 각각의 형태소를 마찬가지로 벡터로 표현하여 이를 Neural Network 를 이용해서 두개의 벡터를 하나의 벡터로 combine 하는 형태를 취한다.</p>
  </li>
  <li>
    <p><strong>Semantics(의미론)</strong>
<img src="downloads/lec1/semantics.png" alt="SEMANTIC" />
기존에는 왼쪽의 그림과 같이 Lambda calculus와 같은 특수한 함수들을 이용한다. 이럴경우 각 단어마다의 similarity에 대한 개념이 포함되지 않는다. 딥러닝을 적용하는 경우는 오른쪽과 같은데 각단어,각구문 모두 이전과 같이 벡터로 표현이 되며, Neural Network를 이용하여 두개의 벡터가 하나의 벡터로 합쳐진다.</p>
  </li>
  <li>
    <p><strong>Sentiment Analysis(감정분석)</strong></p>
  </li>
</ol>

<blockquote>
  <p><em>This movie doesn’t care about cleverness, wit or any other kind of intelligent humor.</em></p>
</blockquote>

<p><img src="downloads/lec1/sentiment.png" alt="sentiment" />
기존의 방법이었다면, <code class="highlighter-rouge">Not</code> 의 축약형인 <code class="highlighter-rouge">n't</code> 를 보고 이 문장이 부정적이다, 또는 일정한 단어 조합을 통해서 문장의 긍정/부정을 판별했지만 위의 그림과 같이 딥러닝을 이용하면 각 단계에서 긍정과 부정을 조합하여 최종적인 감정(현 문장에서는 부정(-))이 도출된다. 위에 적용된 알고리즘은 RNN(Recursive Neural Network)이다.</p>

<p><em>사실 해당 판별 결과에서 <code class="highlighter-rouge">care</code>와 그 오른쪽 이하의 부분을 합친 결과가 0(중립) 이라고 나왔지만 이는 알고리즘상에서 miss 한 부분이다. <code class="highlighter-rouge">care</code> 이하 부분을 읽어보면 +(긍정)에 가깝다고 할 수 있다</em></p>

<h6 id="conclusion">Conclusion</h6>
<p>이번시간에는 NLP의 기본적인 개념과 응용 분야, 딥러닝의 발전 이유와 함께 실제 NLP에서 딥러닝이 어떠한 방식으로 적용되고 있는지 개략적인 부분을 살펴봤다. 딥러닝이 적용되면서 기존 NLP 방식과 달리 벡터를 이용하여 이를 Neural Network 에 적용해 최종적인 원하는 결과를 도출하는 공통적인 모습을 보았다.
다음시간에는 실제 글자와 단어가 어떻게 벡터로 표현이 되는지 word2vec에 대해서 알아보겠다. 끝~</p>


	  ]]></description>
	</item>


</channel>
</rss>
