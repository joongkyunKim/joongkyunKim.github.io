<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>joongkyunKim.github.io</title>
   
   <link>http://joongkyunKim.github.io</link>
   <description>Postings about what i learned and what i have to learn, especially about Deep Learning</description>
   <language>en-uk</language>
   <managingEditor> joongkyunKim</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>[CS224d] Lecture2 : Word2vec</title>
	  <link>//cs224d-lecture2</link>
	  <author>joongkyunKim</author>
	  <pubDate>2017-03-27T00:00:00+09:00</pubDate>
	  <guid>//cs224d-lecture2</guid>
	  <description><![CDATA[
	     <p>이번 포스팅에서는 lecture1:introduction 에 이어서 lecture2: Word vectors에 관해서 내용을 정리해보도록 한다. 우선 지난 시간에 했던 내용을 잠시 상기시켜보면, Natural Language Processing(이하 NLP)와 Deep Learning에 대해서 개략적인 내용을 살펴봤으며, 딥러닝을 NLP에 적용했을 때 기존의 해결 방식과 차이점을 비교해 보았다. 여기서 결정적으로 중요했던 부분은 딥러닝에 NLP를 적용하기 위해서는 글자 하나하나를 벡터의 형태로 나타내야 한다는 공통점을 보였다. 이번 강의에서는 어떻게 글자를 벡터로 나타내는지에 대한 내용이다.</p>

<ul>
  <li>Lecture slide : <a href="http://joongkyunKim.github.io/downloads/lec2/LectureSlide2.pdf">LectureSlide2</a></li>
  <li>Lecture note(lecture1과 동일)  : <a href="http://joongkyunKim.github.io/downloads/lec1/LectureNote1.pdf">Lecturenote1</a></li>
  <li>Video Clip    : <a href="https://www.youtube.com/watch?v=aRqn8t1hLxs&amp;index=2&amp;list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG&amp;t=3327s">2강</a></li>
</ul>

<h3 id="lecture2--word-vectors">Lecture2 : Word vectors</h3>

<p>컴퓨터가 단어의 의미를 나타내기 위해 활용했던 방법으로 WordNet과 같이 상위-하위어의 관계를 보여주는 분류체계(taxonomy)로 나타내거나 동의어 집합을 보여주는 것이 종종 사용되었다.
<img src="downloads/lec2/wordnet.png" alt="WordNet" />
하지만 위의 방식에는 몇몇 문제점이 존재한다. 먼저,’good’ 의 동의어로 나온 adept, expert 등등은 동의어이지만 문맥에 따라 같이 쓰일 수 없다.(<code class="highlighter-rouge">nuance</code>의 차이) 또한 사람들이 하나하나 수작업으로 매칭작업을 진행해줘야하며 단어간의 유사성 계산에 있어서 힘들 수 밖에 없다.</p>

<p>또한 문장 또는 문서에서 해당 단어가 나오는 부분에 1, 아닌 부분에 0으로 만드는 벡터의 경우(아래그림)는 대부분 0으로 구성된 벡터이다. 그래서 아래의 예제와 같이 <code class="highlighter-rouge">motel</code>과 <code class="highlighter-rouge">hotel</code>은 유사한 단어지만 <code class="highlighter-rouge">AND</code> 연산을 시행했을 때 0으로 아무 연관이 없는것 처럼 표현되기도 한다.
<img src="downloads/lec2/onehot.png" alt="onehot" /></p>

<h6 id="window-based-cooccurence-matrix">Window based cooccurence matrix</h6>

<p>이러한 문제점을 해결하기 위해 제시된 방식이 표현하고자 하는 단어의 이웃(neighbors)를 표현하는 방식이다. 이를 이용해서 표현한 단어들의 벡터의 집합을 <code class="highlighter-rouge">window based cooccurence matrix</code> 라 한다. 아래와 같이 3개의 문장이 있다고 가정하자. 해당 예제에서는 window의 길이를 1로 하고, symmetric한 구조를 가진다. 이렇게되면 focus 하고 있는 단어의 좌/우 1개씩을 window로 설정하는 방식이다. 예를들어 <code class="highlighter-rouge">like</code>라는 단어를 보면, <code class="highlighter-rouge">like</code> 좌우 이웃된 단어들은 첫문장에서 <code class="highlighter-rouge">I</code>,<code class="highlighter-rouge">deep</code> 그리고 두번째 문장에서 <code class="highlighter-rouge">I</code>,<code class="highlighter-rouge">NLP</code> 이다. 이 결과가 아래의 매트릭스에서 counting 된것을 확인할 수 있다. 이렇게 계산된 matrix가 아래와 같고 이는 당연스럽게 symmetric한 matrix 일 수 밖에 없다.</p>

<blockquote>
  <p>I like deep learning.<br />
I like NLP.<br />
I enjoy flying.</p>
</blockquote>

<p><img src="downloads/lec2/cooccurence.png" alt="cooccurence" /></p>

<p>하지만, 위의 방식에는 큰 문제점이 있다! 단어 수에 따라 matrix의 크기가 기하급수적으로 늘어날 뿐 아니라, matrix의 차원(dimension)이 커지면서 계산시에 매우 비효율적일 수 있다. 이러한 문제를 해결하기 위해 <code class="highlighter-rouge">차원축소</code> 기법이 필요하다</p>

<h6 id="singular-value-decompositionsvd">차원축소기법 : Singular Value Decomposition(SVD)</h6>
<p>Singular Value Decomposition(이하 SVD) 는 특잇값 분해라고 말하며, 이는 선형대수에서 고유값 분해(eigenvalue decomposition)과 함께 아주 기본적이고 매우 많이 쓰이는 행렬분해 기법이다. $U$는 $AA^T$를 고유값분해(eigendecomposition)해서 얻어진 직교행렬(orthogonal matrix)로 $U$의 열벡터들을 $A$의 left singular vector라 부른다.또한 $V$는 $A^TA$를 고유값분해해서 얻어진 직교행렬로서 $V$의 열벡터들을 $A$의 right singular vector라 부른다.마지막으로, $\Sigma$는 $AA^T$, $A^TA$를 고유값분해해서 나오는 고유값(eigenvalue)들의 square root를 대각원소로 하는 m x n 직사각 대각행렬로 그 대각원소들을 $A$의 특이값(singular value)이라 부른다. (<em>좀더 자세한 내용은 아래 출처인 ‘다크프로그래머’ 블로그를 참조하기 바란다. SVD에 대해 아주 꼼꼼하고 다방면으로 잘 정리된 블로그이다.</em>)</p>

<p>출처: http://darkpgmr.tistory.com/106 (다크프로그래머)</p>

<p>이제 다시 강의내용으로 돌아와서 아래의 SVD 분해 형태를 살펴보자
<img src="downloads/lec2/SVD.png" alt="SVD" />
위의 케이스는 full SVD형태이지만 잘 쓰지 않는다. 아래와 같이 reduced SVD를 일반적으로 쓰게 되는데, 특잇값중에서 0이 아닌 것들을 모아서 좀 더 압축된 형태로 분해하게 된다. (<em>불필요한(덜 중요한) 정보를 감안하지 않는다고 생각하면 될 것 같다.</em>) 이와 같이 축약된 형태의 SVD는 데이터 압축 등에 종종 사용되니 알아두도록 하자!
최종적으로, 우리도 <code class="highlighter-rouge">window based cooccurence matrix</code> 를 축약된 SVD를 이용해서 차원 축소를 시킬것이다.</p>

<p>이 차원 축소 과정을 아래의 파이썬 코드로 실행해 볼 수 있다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">##python 3.0 이상 버전입니다.</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">la</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s">"I"</span><span class="p">,</span> <span class="s">"like"</span><span class="p">,</span><span class="s">"enjoy"</span><span class="p">,</span><span class="s">"deep"</span><span class="p">,</span><span class="s">"learning"</span><span class="p">,</span><span class="s">"NLP"</span><span class="p">,</span><span class="s">"flying"</span><span class="p">,</span><span class="s">"."</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">la</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">U</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">,])</span></code></pre></figure>

<p>이 코드의 결과는 아래와 같이 나온다.
<img src="downloads/lec2/SVDresult.png" alt="SVDresult" /></p>

<script type="math/tex; mode=display">U[0,] =\begin{pmatrix} -0.52412493 \\\ -0.57285914 \\\ 0.0954463 \\\  0.38322849 \\\ -0.17696338 \\\ -0.17609218 \\\ -0.4191856 \\\ -0.05577027\end{pmatrix}</script>

<p>위의 그림을 보면, U행렬을 첫열과 둘째열을 plot으로 나타낸것이다. 기하학적 의미를 간략하게 살펴보면 위에서 구한 <code class="highlighter-rouge">window based cooccurence matrix</code> 에서 단어간의 유사도를 살펴볼 수 있다. 이는 아래 그림의 예제에서 살펴볼 수 있는데, 동일하게 SVD를 통해서 plot한 결과가 같은 단어의 시제끼리 비슷한 위치에 존재하는 것을 알 수 있다.
<img src="downloads/lec2/SVDexp.png" alt="SVDexp" />
출처 : An Improved Model of Seman4c Similarity Based on Lexical Co-Occurrence.Rohde et al. 2005</p>

<p>그리고 이렇게 SVD 과정을 거치게 되면 아래의 벡터와 같이 기존에는 0이 많은 sparse 한 행렬에서 dense 한 행렬로 변환될 수 있다. (<em>dense 한 행렬을 이용하여 모델링 할 경우 robust한 모형이 나올 수 있다.</em>)
하지만, SVD의 단점도 존재한다. 계산 비용이 너무 크게 들기도 하며(매트릭스 크기가 커짐에 따라), 새로운 단어나 문서가 존재하면 새롭게 매르틱스를 구한 후, SVD를 해야한다는 점을 들 수 있다.</p>

<h6 id="word2vec-">word2vec 기초</h6>
<p>자~ 이제 위의 내용을 바탕으로 진짜 이번 강의에서 다루고자 한 word2vec 에 관한 내용을 정리해보자. 이 방법은 기존의 단순한 동시발생(cooccurrence)를 카운팅하는것이 아니라 특정 크기의 window를 기반으로 주변 단어를 예측하고자 하는 방법이다. 우선 방식은 아래의 그림과 같다. 예를들어 window의 길이가 2라고 가정하자. 그러면 아래와 같이 나타낼 수 있다. 검은 줄을 하나하나의 단어라고 하면 파란색의 window와 같이 중심단어를 기준으로 좌우 2개씩 예측을 하고, window가 한칸 넘어가서 빨간색의 window와 같이 예측하고, 이런식으로 문서의 끝까지 진행되는 것을 의미한다.
<img src="downloads/lec2/word2vec.png" alt="word2vec" />
여기서 적용되는 목적함수(objective function)는 중심 단어가 주어졌을 때 context 단어(<em>정확한건지는 모르겠으나 개인적인 이해로는 window 내의 다른 단어가 context 단어라고 생각된다</em>)가 나올 로그 확률을 최대화 하는 함수를 목적함수로 가진다. 여기서 $\theta$ 는 우리가 최적화 하려고하는 모든 변수들을 나타낸다.
<img src="downloads/lec2/word2vecobjfun.png" alt="word2vecobjfun" /></p>

<p>여기에서
$p(w_{t+j} | w_t)$
는 다음과 같이 바꿔서 쓸 수 있다.</p>

<script type="math/tex; mode=display">p(o | c) = \frac{exp(u_o^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)} --- (1)</script>

<p>여기서 o는 outside(or output) 단어의 index(id) 이며, c는 중심단어의 index를 나타낸다. 또한 $u$와 $v$는 각각 outside 단어의 중심단어 벡터와 중심단어의 outside 벡터를 나타낸다..(<em>이부분은 명확히 이해가 되지 않는 부분이다. 더 자세한 벡터에 대한 설명이 없어서 추후 강의를 들어야 이해가 될것으로보인다. 본인이 이해하기엔 u는 예측하고자하는(outside)단어에서의 중심단어를 나타내는 벡터이며, v는 반대로 중심단어에서 예측하고자하는(outside)단어의 벡터를 나타내는 것으로보인다.</em>)</p>

<p>여기서 중요한것은 각각의 단어는 위와 같이 $u$, $v$ 라는 2개의 관점의 벡터를 가진다는점!
(아래는 강의에 나온 표현을 그대로..)</p>

<blockquote>
  <p>1 vector when we represent it as an outside word -&gt; $u$ <br />
  1 vector as we trying to predict the outside word -&gt; $v$</p>
</blockquote>

<p>지금까지 목적함수와 목적함수에 들어가있는 로그 확률의 식을 알아봤다. 이제 우리는 목적함수의 최적화 문제를 풀어야한다. 해당 목적함수는 로그 확률의 최대값을 구하는 것이며, 여기에서 우리는 <code class="highlighter-rouge">gradient</code> 방식을 사용할 것이다. <code class="highlighter-rouge">gradient</code> 방식은 대게 목적함수를 최소화(minimize) 할때 <code class="highlighter-rouge">gradient descent</code> 방식을 많이 사용한다. 수식을 사용하지 않고 직관적으로 설명을 하면, 특정 목적함수의 <code class="highlighter-rouge">gradient</code>(쉽게 기울기) 를 이용해서 이 기울기의 반대방향으로 계속해서 이동하다보면 극소(이는 local minimum일 수도, global minimum일 수도)가 되는 곳에 도달한다는 알고리즘이다(<em>gradient descent 방법에 대해 자세히 알고 싶다면 <a href="http://darkpgmr.tistory.com/133">Gradient Descent  탐색 방법</a>을 참고하기 바란다</em>). <code class="highlighter-rouge">Gradient</code> 최적화 방법을 사용하기 위해 우리는 목적함수의 <code class="highlighter-rouge">gradient</code>를 구할 줄 알아야 한다.
여기서 기본적으로 <code class="highlighter-rouge">chain rule</code> 을 알아야 하는데 간단하게 언급하면 $y=f(u)$ 이고 $u=g(x)$ 일때 즉, $y = f(g(x))$ 일때,</p>

<script type="math/tex; mode=display">\begin{equation}
\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}  
\end{equation}</script>

<p>로 나타 낼 수 있는것이다. 이는 고등학교때 합성함수의 미분을 배우면서 지나치며 배워온 지식이니 간단하다!
자 이제 목적함수의 <code class="highlighter-rouge">gradient</code>를 구해보자. 위에서 정의한 (1) 식에서 $v_c$로 편미분을 한다. $v_c$로 편미분을 하는 것은 위에서 언급한 것과 같이 $v$ 벡터가 우리가 예측하고자 하는 단어의 벡터를 나타내고 있으며 이것이 파라미터인 $\theta$에 depend한 변수이므로 이에 대한 <code class="highlighter-rouge">gradient</code>를 구하는 것이다.</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial v_c}(\log exp(u_o^T v_c) - \log \sum_{w=1}^W exp(u_w^T v_c))</script>

<script type="math/tex; mode=display">= \frac{\partial}{\partial v_c}(u_o^T v_c) - \frac{\partial}{\partial v_c}\color{red}{\log} \color{blue}{(\sum_{w=1}^W exp(u_w^T v_c)})</script>

<script type="math/tex; mode=display">= (u_o) - \frac{\partial}{\partial v_c}\color{red}{f}\color{blue}{(g(v_c))}</script>

<p>여기서 $u_o$ 를 제외한 오른쪽 부분을 따로 <code class="highlighter-rouge">chain rule</code>을 이용해서 다음과 같이 구할 수 있다.</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial v_c}\color{red}{f}\color{blue}{(z)} =
\frac{\partial \color{red}{f}\color{blue}{(z)}}{\partial \color{blue}{z}}
\frac{\partial \color{blue}{g(v_c)}}{\partial v_c} =
\frac{1}{\sum_{w=1}^W exp(u_w^T v_c)} \frac{\partial}{\partial v_c}{(\sum_{x=1}^W exp(u_x^T v_c)})</script>

<script type="math/tex; mode=display">= \frac{1}{\sum_{w=1}^W exp(u_w^T v_c)} {\sum_{x=1}^W exp(u_x^T v_c)} \frac{\partial}{\partial v_c}(u_x^T v_c)
= \frac{\sum_{x=1}^W exp(u_x^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)} (u_x)</script>

<p>위의 식을 최종적으로 정리하면 아래와 같이 간략하게 나타낼 수 있다.</p>

<script type="math/tex; mode=display">u_o - \sum_{x=1}^W \frac{exp(u_x^T v_c)}{\sum_{w=1}^W exp(u_w^T v_c)}  u_x = u_o - \sum_{x=1}^W p(x | c) u_x</script>

<p>최종적으로 목적함수가 위와 같이 <code class="highlighter-rouge">gradient function</code> 으로 나타났다. 여기서 $W$ 는 전체 단어의 수로 수백만개까지 갈 수 있어서 강의에서는 <code class="highlighter-rouge">very expensive function to compute</code> 이라 얘기한다. 그래서 이는 approximation 과 normalization 이 필요하다. (<em>이는 과제에서 다뤄질것으로 보인다..</em>)</p>

<h6 id="summary">Summary</h6>
<p>2강에서는 단어를 어떻게 벡터의 형태로 나타낼 수 있는지에 대해서 간략하게 알아봤다. 벡터로 나타내는 방식에는 <code class="highlighter-rouge">window based cooccurence matrix</code> 가 있으며, 이 매트릭스를 이용해 SVD를 진행하여 기하학적 의미도 살펴봤다. 그리고 <code class="highlighter-rouge">word2vec</code> 알고리즘을 진행하기 위해 목적함수와 확률식, 최적화를 위한 <code class="highlighter-rouge">gradient</code> 도출까지 진행해보았다. 이러한 기초 지식을 가지고 다음강에서부터는 word2vec를 통해 나타난 결과의 의미와, <code class="highlighter-rouge">GloVe</code> 와 같은 다른 형태의 word vector 표현 방식을 알아보도록 하겠다. 끝~</p>

<p>$x$에 대한 이차 방정식 $ax^2 + bx + c = 0$의 해는 다음과 같다.
 \[
 x = \frac{-b \pm \sqrt{b^2-4ac}}{2a}
 \]</p>


	  ]]></description>
	</item>

	<item>
	  <title>[CS224d] Lecture1 : Intro to NLP&Deep Learning</title>
	  <link>//cs224d-lecture1</link>
	  <author>joongkyunKim</author>
	  <pubDate>2017-03-22T21:30:00+09:00</pubDate>
	  <guid>//cs224d-lecture1</guid>
	  <description><![CDATA[
	     <p>Deep Learning 학습을 위해 CS224d 강의를 보고 강의의 복습 및 정리를 위해 오늘부터 Review 내용을 정리하기로 했다. CS224d : Deep Learning for Natrual Language Processing(이하 NLP) 강의는 Stanford University에서 진행되었고 지속해서 진행되고 있는 강의로 해외뿐만아니라 우리나라에서도 딥러닝 초보 학습자들에게 인기를 모은 강의이다. 현재 2017년에는 CS224n 강의가 새롭게 진행중이지만, 강의 비디오를 볼 수 없는 관계로 youtube에서 강의를 볼 수 있는 2016년 강의인 CS224d를 학습하고 정리한다.</p>

<p>오늘은 Lecture1인 Intro to NLP and Deep Learning 에 대해서 알아보기로 한다.</p>

<ul>
  <li>Lecture slide : <a href="http://joongkyunKim.github.io/downloads/lec1/LectureSlide1.pdf">LectureSlide1</a></li>
  <li>Lecture note  : <a href="http://joongkyunKim.github.io/downloads/lec1/LectureNote1.pdf">Lecturenote1</a></li>
  <li>Video Clip    : <a href="https://www.youtube.com/watch?v=Qy0oEkCZkBI&amp;list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG&amp;index=1">1강</a></li>
</ul>

<h3 id="lecture1--introduction">Lecture1 : Introduction</h3>

<p>첫 강좌에서는 기본적인 NLP의 개념, 응용과 딥러닝의 개념 및 응용에 대해서 설명하고있다.</p>

<h6 id="nlpnatural-language-processing">NLP(Natural Language Processing)</h6>
<p>NLP는 크게 computer science, artificial intelligence(AI), linguistics(언어학) 분야가 접합된 것으로, 결국엔 컴퓨터가 자연언어를 ‘이해’ 할 수 있게 처리를 해주는것을 의미한다.
본 수업에서는 주로 Syntactic analysis(구문 분석)과 Semantic analysis(의미, 맥락 분석)에 포커스를 맞춰서 수업이 진행된다.</p>

<p>NLP의 응용분야는 spelling checking, keyword search 등과 같이 매우 단순한 분야부터 글의 긍정/부정 분류, 기계 번역, 질문 응답과 같은 어려운 분야까지 다양하게 응용될 수 있다.</p>

<p>NLP에서의 어려운점은 기계는 우리의 언어를 문맥상으로 파악하기 힘든것에 있다. 아래의 예제를 살펴보자</p>

<blockquote>
  <p><em>Kim hit Park and then she [fell/ran]</em></p>
</blockquote>

<p>해당 예제의 경우,  <code class="highlighter-rouge">fell</code>이 선택될 경우에는  <code class="highlighter-rouge">she</code>가 지칭하는 것이  <code class="highlighter-rouge">Park</code>이 되지만 <code class="highlighter-rouge">ran</code>의 경우에는 <code class="highlighter-rouge">she</code>가 지칭하는것이 <code class="highlighter-rouge">Kim</code>이 된다. 이와 같이 어느 동사가 오느냐에 따라 지칭(refer)하는 것이 달라질 수 있으며, 인간은 이 문장을 문맥상으로 이해하여 찾아낼 수 있지만 컴퓨터는 그렇게 하지 못한다. 또한, 이중적인 문장과 같은 경우도 컴퓨터가 올바른 답을 내놓지 못할 수도 있다.</p>

<h6 id="dldeep-learning">DL(Deep Learning)</h6>
<p>딥러닝은 머신러닝(Machine Learning)의 분야 중 하나로, 요즘 computer vision, speech recognition 등과 같은 분야에서 각광받고 있다. 대부분의 머신러닝 방법은 well defined 된 human-designed representation와 input features에 잘 적용이 되며, 최종 예측을 위해 parameter들을 최적화하는 작업이 주가 된다. 반면, 딥러닝의 경우는 이와 달리 human-designed된 feature도 사용하지 않고, raw한 데이터를 넣어서 features 까지 learning해서 구해버린다.
바로 이점이 딥러닝이 요새 각광받고 있는 이유가 될것이다. 특정 분야에 대하여 features를 만들어서 이를 튜닝하려면 해당 분야의 전문 지식(Domain Knowledge)가 상당히 필요하지만, 딥러닝을 이용한다면 데이터를 통해서 모델이 <strong>learn feature itself</strong> 를 통해 features를 만들어주므로, 손수 힘들게 features를 만들필요가 없는것이다!!
이를 위해서는 상당한 computing power (learning time을 줄이기 위해)가 필요한데, 최근 CPU/GPU의 발전과 함께 물만난 고기마냥 딥러닝이 높은 성능을 보여주고 있다.</p>

<h6 id="deep-learning--nlp--deep-nlp">Deep Learning + NLP = Deep NLP</h6>
<p>representation learning 과 deep learning 방법을 이용해서 NLP 문제들을 더 좋은 성능으로 풀어보기 위해 NLP에 딥러닝이 적용되었다. 이를 통해서 NLP 각 영역에서 좋은 성능 향상을 이뤄냈다.</p>

<ol>
  <li>
    <p><strong>Phonology(음운체계)</strong>
<img src="downloads/lec1/phonemes.png" alt="PHONEMES" />
기존에는 위의 표와 같이 정해진 음운체계표를 만들어서 적용을 했지만, 딥러닝이 적용된 현재에는 이러한 음운을 예측하기위해 sound features를 training하고, 결과를 숫자 벡터로 표현한다.</p>
  </li>
  <li>
    <p><strong>Morphology(형태론)</strong>
<img src="downloads/lec1/morphemes.png" alt="morphemes" />
<img src="downloads/lec1/morphology.png" alt="MORPHOLOGY" />
기존에는 위의 표와 같이 단어를 형태소(접두사/어근/접미사 등)의 형태로 나눠서 구분지었지만, 딥러닝을 적용하면서 아래와 같이 각각의 형태소를 마찬가지로 벡터로 표현하여 이를 Neural Network 를 이용해서 두개의 벡터를 하나의 벡터로 combine 하는 형태를 취한다.</p>
  </li>
  <li>
    <p><strong>Semantics(의미론)</strong>
<img src="downloads/lec1/semantics.png" alt="SEMANTIC" />
기존에는 왼쪽의 그림과 같이 Lambda calculus와 같은 특수한 함수들을 이용한다. 이럴경우 각 단어마다의 similarity에 대한 개념이 포함되지 않는다. 딥러닝을 적용하는 경우는 오른쪽과 같은데 각단어,각구문 모두 이전과 같이 벡터로 표현이 되며, Neural Network를 이용하여 두개의 벡터가 하나의 벡터로 합쳐진다.</p>
  </li>
  <li>
    <p><strong>Sentiment Analysis(감정분석)</strong></p>
  </li>
</ol>

<blockquote>
  <p><em>This movie doesn’t care about cleverness, wit or any other kind of intelligent humor.</em></p>
</blockquote>

<p><img src="downloads/lec1/sentiment.png" alt="sentiment" />
기존의 방법이었다면, <code class="highlighter-rouge">Not</code> 의 축약형인 <code class="highlighter-rouge">n't</code> 를 보고 이 문장이 부정적이다, 또는 일정한 단어 조합을 통해서 문장의 긍정/부정을 판별했지만 위의 그림과 같이 딥러닝을 이용하면 각 단계에서 긍정과 부정을 조합하여 최종적인 감정(현 문장에서는 부정(-))이 도출된다. 위에 적용된 알고리즘은 RNN(Recursive Neural Network)이다.</p>

<p><em>사실 해당 판별 결과에서 <code class="highlighter-rouge">care</code>와 그 오른쪽 이하의 부분을 합친 결과가 0(중립) 이라고 나왔지만 이는 알고리즘상에서 miss 한 부분이다. <code class="highlighter-rouge">care</code> 이하 부분을 읽어보면 +(긍정)에 가깝다고 할 수 있다</em></p>

<h6 id="conclusion">Conclusion</h6>
<p>이번시간에는 NLP의 기본적인 개념과 응용 분야, 딥러닝의 발전 이유와 함께 실제 NLP에서 딥러닝이 어떠한 방식으로 적용되고 있는지 개략적인 부분을 살펴봤다. 딥러닝이 적용되면서 기존 NLP 방식과 달리 벡터를 이용하여 이를 Neural Network 에 적용해 최종적인 원하는 결과를 도출하는 공통적인 모습을 보았다.
다음시간에는 실제 글자와 단어가 어떻게 벡터로 표현이 되는지 word2vec에 대해서 알아보겠다. 끝~</p>


	  ]]></description>
	</item>


</channel>
</rss>
